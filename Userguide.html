<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>User’s guide &mdash; MILA Technical Documentation latest documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/documentation_options.js"></script>
        <script src="_static/documentation_options_fix.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AI tooling and methodology handbook" href="Handbook.html" />
    <link rel="prev" title="Computational resources outside of Mila" href="Extra_compute.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/image.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                latest
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html">Purpose of this documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html#contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">General theory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html">What is a computer cluster?</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#parts-of-a-computing-cluster">Parts of a computing cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#unix">UNIX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#the-workload-manager">The workload manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#processing-data">Processing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#software-on-the-cluster">Software on the cluster</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Systems and services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Information.html">Computing infrastructure and policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="Extra_compute.html">Computational resources outside of Mila</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How-tos and Guides</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">User’s guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#logging-in-to-the-cluster">Logging in to the cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ssh-login">SSH Login</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mila-init">mila init</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ssh-config">SSH Config</a></li>
<li class="toctree-l3"><a class="reference internal" href="#passwordless-login">Passwordless login</a></li>
<li class="toctree-l3"><a class="reference internal" href="#connecting-to-compute-nodes">Connecting to compute nodes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-your-code">Running your code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#slurm-commands-guide">SLURM commands guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submitting-jobs">Submitting jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-submission-arguments">Job submission arguments</a></li>
<li class="toctree-l4"><a class="reference internal" href="#checking-job-status">Checking job status</a></li>
<li class="toctree-l4"><a class="reference internal" href="#removing-a-job">Removing a job</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#partitioning">Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#information-on-partitions-nodes">Information on partitions/nodes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#useful-commands">Useful Commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="#special-gpu-requirements">Special GPU requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cpu-only-jobs">CPU-only jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-script">Example script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#portability-concerns-and-solutions">Portability concerns and solutions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#managing-your-environments">Managing your environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#virtual-environments">Virtual environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pip-virtualenv">Pip/Virtualenv</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conda">Conda</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-modules">Using Modules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-module-command">The module command</a></li>
<li class="toctree-l4"><a class="reference internal" href="#available-software">Available Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="#default-package-location">Default package location</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#on-using-containers">On using containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Singularity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-singularity">What is Singularity?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#links-to-official-documentation">Links to official documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#overview-of-the-steps-used-in-practice">Overview of the steps used in practice</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nope-not-on-macos">Nope, not on MacOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#where-to-build-images">Where to build images</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#building-the-containers">Building the containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#first-way-build-and-use-a-sandbox">First way: Build and use a sandbox</a></li>
<li class="toctree-l4"><a class="reference internal" href="#second-way-use-recipes">Second way: Use recipes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-containers-on-clusters">Using containers on clusters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use-containers-on-clusters">How to use containers on clusters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sharing-data-with-acls">Sharing Data with ACLs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#viewing-and-verifying-acls">Viewing and Verifying ACLs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#contributing-datasets">Contributing datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#publicly-share-a-mila-dataset">Publicly share a Mila dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">Academic Torrent</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">Google Drive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#digital-object-identifier-doi">Digital Object Identifier (DOI)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-transmission-using-globus-connect-personal">Data Transmission using Globus Connect Personal</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jupyterhub">JupyterHub</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#access-mila-storage-in-jupyterlab">Access Mila Storage in JupyterLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-slurm-usage-and-multiple-gpu-jobs">Advanced SLURM usage and Multiple GPU jobs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#handling-preemption">Handling preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="#packing-jobs">Packing jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-gpu-between-processes">Sharing a GPU between processes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-1process-gpu">Sharing a node with multiple GPU 1process/GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu">Sharing a node with multiple GPU &amp; multiple processes/GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#frequently-asked-questions-faqs">Frequently asked questions (FAQs)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#connection-ssh-issues">Connection/SSH issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#shell-issues">Shell issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-do-i-change-my-shell">How do I change my shell ?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-issues">SLURM issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-get-an-interactive-shell-on-the-cluster">How can I get an interactive shell on the cluster ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-reset-my-cluster-password">How can I reset my cluster password ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">srun: error: –mem and –mem-per-cpu are mutually exclusive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-see-where-and-if-my-jobs-are-running">How can I see where and if my jobs are running ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">Unable to allocate resources: Invalid account or account/partition combination specified</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-do-i-cancel-a-job">How do I cancel a job?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">How can I access a node on which one of my jobs is running ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#where-do-i-put-my-data-during-a-job">Where do I put my data during a job ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fork-retry-resource-temporarily-unavailable">fork: retry: Resource temporarily unavailable</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-issues">PyTorch issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Handbook.html">AI tooling and methodology handbook</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extras</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Audio_video.html">Audio and video resources at Mila</a></li>
<li class="toctree-l1"><a class="reference internal" href="VSCode.html">Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="IDT.html">Who, what, where is IDT</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MILA Technical Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>User’s guide</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mila-iqia/mila-docs/blob/master/docs/Userguide.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="user-s-guide">
<span id="userguide"></span><h1>User’s guide<a class="headerlink" href="#user-s-guide" title="Permalink to this heading"></a></h1>
<p>…or <em>IDT’s list of opinionated howtos</em></p>
<p>This section seeks to provide users of the Mila infrastructure with practical
knowledge, tips and tricks and example commands.</p>
<section id="logging-in-to-the-cluster">
<h2>Logging in to the cluster<a class="headerlink" href="#logging-in-to-the-cluster" title="Permalink to this heading"></a></h2>
<p>To access the Mila Cluster clusters, you will need a Mila account. Please contact
Mila systems administrators if you don’t have it already. Our IT support service
is available here: <a class="reference external" href="https://it-support.mila.quebec/">https://it-support.mila.quebec/</a></p>
<p>You will also need to complete and return an IT Onboarding Training to get
access to the cluster.  Please refer to the Mila Intranet for more
informations:
<a class="reference external" href="https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training">https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training</a></p>
<p><strong>IMPORTANT</strong> : Your access to the Cluster is granted based on your status at
Mila (for students, your status is the same as your main supervisor’ status),
and on the duration of your stay, set during the creation of your account. The
following have access to the cluster : <strong>Current Students of Core Professors -
Core Professors - Staff</strong></p>
<section id="ssh-login">
<h3>SSH Login<a class="headerlink" href="#ssh-login" title="Permalink to this heading"></a></h3>
<p>You can access the Mila cluster via ssh:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1"><span class="c1"># Generic login, will send you to one of the 4 login nodes to spread the load</span></span>
<span class="prompt1">ssh &lt;user&gt;@login.server.mila.quebec -p <span class="m">2222</span></span>
<span class="prompt1"></span>
<span class="prompt1"><span class="c1"># To connect to a specific login node, X in [1, 2, 3, 4]</span></span>
<span class="prompt1">ssh &lt;user&gt;@login-X.login.server.mila.quebec -p <span class="m">2222</span></span>
</pre></div></div><p>Four login nodes are available and accessible behind a load balancer. At each
connection, you will be redirected to the least loaded login-node.</p>
<p>The ECDSA, RSA and ED25519 fingerprints for Mila’s login nodes are:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>SHA256:baEGIa311fhnxBWsIZJ/zYhq2WfCttwyHRKzAb8zlp8 <span class="o">(</span>ECDSA<span class="o">)</span>
SHA256:Xr0/JqV/+5DNguPfiN5hb8rSG+nBAcfVCJoSyrR0W0o <span class="o">(</span>RSA<span class="o">)</span>
SHA256:gfXZzaPiaYHcrPqzHvBi6v+BWRS/lXOS/zAjOKeoBJg <span class="o">(</span>ED25519<span class="o">)</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Login nodes are merely <em>entry points</em> to the cluster. They give you access
to the compute nodes and to the filesystem, but they are not meant to run
anything heavy. Do <strong>not</strong> run compute-heavy programs on these nodes,
because in doing so you could bring them down, impeding cluster access for
everyone.</p>
<p>This means no training or experiments, no compiling programs, no Python
scripts, but also no <code class="docutils literal notranslate"><span class="pre">zip</span></code> of a large folder or anything that demands a
sustained amount of computation.</p>
<p><strong>Rule of thumb:</strong> never run a program that takes more than a few seconds on
a login node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In a similar vein, you should not run VSCode remote SSH instances directly
on login nodes, because even though they are typically not very
computationally expensive, when many people do it, they add up! See
<a class="reference internal" href="VSCode.html#visual-studio-code"><span class="std std-ref">Visual Studio Code</span></a> for specific instructions.</p>
</div>
</div>
</section>
<section id="mila-init">
<h3>mila init<a class="headerlink" href="#mila-init" title="Permalink to this heading"></a></h3>
<p>To make it easier to set up a productive environment, Mila publishes the
<a class="reference external" href="https://github.com/mila-iqia/milatools">milatools</a> package, which defines a <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code> command which will
automatically perform some of the below steps for you. You can install it with
<code class="docutils literal notranslate"><span class="pre">pip</span></code> and use it, provided your Python version is at least 3.8:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ pip install milatools
$ mila init
</pre></div>
</div>
</section>
<section id="ssh-config">
<h3>SSH Config<a class="headerlink" href="#ssh-config" title="Permalink to this heading"></a></h3>
<p>The login nodes support the following authentication mechanisms:
<code class="docutils literal notranslate"><span class="pre">publickey,keyboard-interactive</span></code>.  If you would like to set an entry in your
<code class="docutils literal notranslate"><span class="pre">.ssh/config</span></code> file, please use the following recommendation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Host mila
    User YOUR-USERNAME
    Hostname login.server.mila.quebec
    PreferredAuthentications publickey,keyboard-interactive
    Port <span class="m">2222</span>
    ServerAliveInterval <span class="m">120</span>
    ServerAliveCountMax <span class="m">5</span>
</pre></div>
</div>
<p>Then you can simply write <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">mila</span></code> to connect to a login node. You will also
be able to use <code class="docutils literal notranslate"><span class="pre">mila</span></code> with <code class="docutils literal notranslate"><span class="pre">scp</span></code>, <code class="docutils literal notranslate"><span class="pre">rsync</span></code> and other such programs.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can run commands on the login node with <code class="docutils literal notranslate"><span class="pre">ssh</span></code> directly, for example
<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">mila</span> <span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">'$USER'</span></code> (remember to put single quotes around any
<code class="docutils literal notranslate"><span class="pre">$VARIABLE</span></code> you want to evaluate on the remote side, otherwise it will be
evaluated locally before ssh is even executed).</p>
</div>
</section>
<section id="passwordless-login">
<h3>Passwordless login<a class="headerlink" href="#passwordless-login" title="Permalink to this heading"></a></h3>
<p>To save you some repetitive typing it is highly recommended to set up public
key authentication, which means you won’t have to enter your password every time
you connect to the cluster.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ON YOUR LOCAL MACHINE</span>
<span class="c1"># You might already have done this in the past, but if you haven&#39;t:</span>
ssh-keygen  <span class="c1"># Press ENTER 3x</span>

<span class="c1"># Copy your public key over to the cluster</span>
<span class="c1"># You will need to enter your password</span>
ssh-copy-id mila
</pre></div>
</div>
</section>
<section id="connecting-to-compute-nodes">
<h3>Connecting to compute nodes<a class="headerlink" href="#connecting-to-compute-nodes" title="Permalink to this heading"></a></h3>
<p>If (and only if) you have a job running on compute node “cnode”, you are
allowed to SSH to it directly, if for some reason you need a second terminal.
That session will be automatically ended when your job is relinquished.</p>
<p>First, however, you need to have
password-less ssh either with a key present in your home or with an
<code class="docutils literal notranslate"><span class="pre">ssh-agent</span></code>. To generate a key pair on the login node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ON A LOGIN NODE</span>
ssh-keygen  <span class="c1"># Press ENTER 3x</span>
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod <span class="m">600</span> ~/.ssh/authorized_keys
chmod <span class="m">700</span> ~/.ssh
</pre></div>
</div>
<p>Then from the login node you can write <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">&lt;node&gt;</span></code>. From your local
machine, you can use <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-J</span> <span class="pre">mila</span> <span class="pre">USERNAME&#64;&lt;node&gt;</span></code> (-J represents a “jump”
through the login node, necessary because the compute nodes are behind a
firewall).</p>
<p>If you wish, you may also add the following wildcard rule in your <code class="docutils literal notranslate"><span class="pre">.ssh/config</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Host *.server.mila.quebec !*login.server.mila.quebec
    HostName %h
    User YOUR-USERNAME
    ProxyJump mila
</pre></div>
</div>
<p>This will let you connect to a compute node with <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">&lt;node&gt;.server.mila.quebec</span></code>.</p>
</section>
</section>
<section id="running-your-code">
<h2>Running your code<a class="headerlink" href="#running-your-code" title="Permalink to this heading"></a></h2>
<section id="slurm-commands-guide">
<h3>SLURM commands guide<a class="headerlink" href="#slurm-commands-guide" title="Permalink to this heading"></a></h3>
<section id="basic-usage">
<h4>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this heading"></a></h4>
<p>The SLURM <a class="reference external" href="https://slurm.schedmd.com/documentation.html">documentation</a>
provides extensive information on the available commands to query the cluster
status or submit jobs.</p>
<p>Below are some basic examples of how to use SLURM.</p>
</section>
<section id="submitting-jobs">
<h4>Submitting jobs<a class="headerlink" href="#submitting-jobs" title="Permalink to this heading"></a></h4>
<section id="batch-job">
<h5>Batch job<a class="headerlink" href="#batch-job" title="Permalink to this heading"></a></h5>
<p>In order to submit a batch job, you have to create a script containing the main
command(s) you would like to execute on the allocated resources/nodes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --job-name=test</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --output=job_output.txt</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --error=job_error.txt</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --ntasks=1</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --time=10:00</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --mem=100Gb</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span>module load python/3.5
<span class="linenos">10</span>python my_script.py
</pre></div>
</div>
<p>Your job script is then submitted to SLURM with <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sbatch.html">ref.</a>)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt2:before {
  content: " ";
}
</style><span class="prompt1">sbatch job_script</span>
<span class="prompt2">sbatch: Submitted batch job <span class="m">4323674</span></span>
</pre></div></div><p>The <em>working directory</em> of the job will be the one where your executed <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Slurm directives can be specified on the command line alongside <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> or
inside the job script with a line starting with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>.</p>
</div>
</section>
<section id="interactive-job">
<h5>Interactive job<a class="headerlink" href="#interactive-job" title="Permalink to this heading"></a></h5>
<p>Workload managers usually run batch jobs to avoid having to watch its
progression and let the scheduler run it as soon as resources are available. If
you want to get access to a shell while leveraging cluster resources, you can
submit an interactive jobs where the main executable is a shell with the
<code class="docutils literal notranslate"><span class="pre">srun/salloc</span></code> (<a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a>/<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a>) commands</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">salloc</span>
</pre></div></div><p>Will start an interactive job on the first node available with the default
resources set in SLURM (1 task/1 CPU).  <code class="docutils literal notranslate"><span class="pre">srun</span></code> accepts the same arguments as
<code class="docutils literal notranslate"><span class="pre">sbatch</span></code> with the exception that the environment is not passed.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To pass your current environment to an interactive job, add
<code class="docutils literal notranslate"><span class="pre">--preserve-env</span></code> to <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">salloc</span></code> can also be used and is mostly a wrapper around <code class="docutils literal notranslate"><span class="pre">srun</span></code> if provided
without more info but it gives more flexibility if for example you want to get
an allocation on multiple nodes.</p>
</section>
</section>
<section id="job-submission-arguments">
<h4>Job submission arguments<a class="headerlink" href="#job-submission-arguments" title="Permalink to this heading"></a></h4>
<p>In order to accurately select the resources for your job, several arguments are
available. The most important ones are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 74%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Argument</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-n, –ntasks=&lt;number&gt;</p></td>
<td><p>The number of task in your script, usually =1</p></td>
</tr>
<tr class="row-odd"><td><p>-c, –cpus-per-task=&lt;ncpus&gt;</p></td>
<td><p>The number of cores for each task</p></td>
</tr>
<tr class="row-even"><td><p>-t, –time=&lt;time&gt;</p></td>
<td><p>Time requested for your job</p></td>
</tr>
<tr class="row-odd"><td><p>–mem=&lt;size[units]&gt;</p></td>
<td><p>Memory requested for all your tasks</p></td>
</tr>
<tr class="row-even"><td><p>–gres=&lt;list&gt;</p></td>
<td><p>Select generic resources such as GPUs for your job: <code class="docutils literal notranslate"><span class="pre">--gres=gpu:GPU_MODEL</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Always consider requesting the adequate amount of resources to improve the
scheduling of your job (small jobs always run first).</p>
</div>
</section>
<section id="checking-job-status">
<h4>Checking job status<a class="headerlink" href="#checking-job-status" title="Permalink to this heading"></a></h4>
<p>To display <em>jobs</em> currently in queue, use <code class="docutils literal notranslate"><span class="pre">squeue</span></code> and to get only your jobs type</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">squeue -u <span class="nv">$USER</span></span>
<span class="prompt2">JOBID   USER          NAME    ST  START_TIME         TIME NODES CPUS TRES_PER_NMIN_MEM NODELIST <span class="o">(</span>REASON<span class="o">)</span> COMMENT</span>
<span class="prompt2"><span class="m">133</span>     my_username   myjob   R   <span class="m">2019</span>-03-28T18:33   <span class="m">0</span>:50     <span class="m">1</span>    <span class="m">2</span>        N/A  7000M node1 <span class="o">(</span>None<span class="o">)</span> <span class="o">(</span>null<span class="o">)</span></span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The maximum number of jobs able to be submitted to the system per user is 1000 (MaxSubmitJobs=1000)
at any given time from the given association. If this limit is reached, new submission requests
will be denied until existing jobs in this association complete.</p>
</div>
</section>
<section id="removing-a-job">
<h4>Removing a job<a class="headerlink" href="#removing-a-job" title="Permalink to this heading"></a></h4>
<p>To cancel your job simply use <code class="docutils literal notranslate"><span class="pre">scancel</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scancel <span class="m">4323674</span></span>
</pre></div></div></section>
</section>
<section id="partitioning">
<h3>Partitioning<a class="headerlink" href="#partitioning" title="Permalink to this heading"></a></h3>
<p>Since we don’t have many GPUs on the cluster, resources must be shared as fairly
as possible.  The <code class="docutils literal notranslate"><span class="pre">--partition=/-p</span></code> flag of SLURM allows you to set the
priority you need for a job.  Each job assigned with a priority can preempt jobs
with a lower priority: <code class="docutils literal notranslate"><span class="pre">unkillable</span> <span class="pre">&gt;</span> <span class="pre">main</span> <span class="pre">&gt;</span> <span class="pre">long</span></code>. Once preempted, your job is
killed without notice and is automatically re-queued on the same partition until
resources are available. (To leverage a different preemption mechanism, see the
<a class="reference internal" href="#advanced-preemption"><span class="std std-ref">Handling preemption</span></a>)</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 34%" />
<col style="width: 34%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Max Resource Usage</p></th>
<th class="head"><p>Max Time</p></th>
<th class="head"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>–partition=unkillable</p></td>
<td><p>1 GPU, 6 CPUs, mem=32G</p></td>
<td><p>2 days</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>–partition=main</p></td>
<td><p>2 GPUs, 8 CPUs, mem=48G</p></td>
<td><p>5 days</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>–partition=long</p></td>
<td><p>no limit of resources</p></td>
<td><p>7 days</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>For instance, to request an unkillable job with 1 GPU, 4 CPUs, 10G of RAM and
12h of computation do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch --gres<span class="o">=</span>gpu:1 -c <span class="m">4</span> --mem<span class="o">=</span>10G -t <span class="m">12</span>:00:00 --partition<span class="o">=</span>unkillable &lt;job.sh&gt;</span>
</pre></div></div><p>You can also make it an interactive job using <code class="docutils literal notranslate"><span class="pre">salloc</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">salloc --gres<span class="o">=</span>gpu:1 -c <span class="m">4</span> --mem<span class="o">=</span>10G -t <span class="m">12</span>:00:00 --partition<span class="o">=</span>unkillable</span>
</pre></div></div><p>The Mila cluster has many different types of nodes/GPUs. To request a specific
type of node/GPU, you can add specific feature requirements to your job
submission command.</p>
<p>To access those special nodes you need to request them explicitly by adding the
flag <code class="docutils literal notranslate"><span class="pre">--constraint=&lt;name&gt;</span></code>.  The full list of nodes in the Mila Cluster can be
accessed <a class="reference internal" href="Information.html#node-profile-description"><span class="std std-ref">Node profile description</span></a>.</p>
<p><em>Example:</em></p>
<p>To request a Power9 machine</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch -c <span class="m">4</span> --constraint<span class="o">=</span>power9</span>
</pre></div></div><p>To request a machine with 2 GPUs using NVLink, you can use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch -c <span class="m">4</span> --gres<span class="o">=</span>gpu:2 --constraint<span class="o">=</span>nvlink</span>
</pre></div></div><table class="docutils align-default">
<colgroup>
<col style="width: 37%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Particularities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>x86_64 (Default)</p></td>
<td><p>Regular nodes</p></td>
</tr>
<tr class="row-odd"><td><p>Power9</p></td>
<td><p><a class="reference internal" href="Information.html#power9-nodes"><span class="std std-ref">Power9</span></a> CPUs (incompatible with x86_64 software)</p></td>
</tr>
<tr class="row-even"><td><p>12GB/16GB/24GB/32GB/48GB</p></td>
<td><p>Request a specific amount of <em>GPU</em> memory</p></td>
</tr>
<tr class="row-odd"><td><p>maxwell/pascal/volta/tesla/turing/kepler</p></td>
<td><p>Request a specific <em>GPU</em> architecture</p></td>
</tr>
<tr class="row-even"><td><p>nvlink</p></td>
<td><p>Machine with GPUs using the NVLink technology</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You don’t need to specify <em>x86_64</em> when you add a constraint as it is added
by default ( <code class="docutils literal notranslate"><span class="pre">nvlink</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">x86_64&amp;nvlink</span></code> )</p>
</div>
<section id="information-on-partitions-nodes">
<h4>Information on partitions/nodes<a class="headerlink" href="#information-on-partitions-nodes" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sinfo.html">ref.</a>) provides most of the
information about available nodes and partitions/queues to submit jobs to.</p>
<p>Partitions are a group of nodes usually sharing similar features. On a
partition, some job limits can be applied which will override those asked for a
job (i.e. max time, max CPUs, etc…)</p>
<p>To display available <em>partitions</em>, simply use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sinfo</span>
<span class="prompt2">PARTITION AVAIL TIMELIMIT NODES STATE  NODELIST</span>
<span class="prompt2">batch     up     infinite     <span class="m">2</span> alloc  node<span class="o">[</span><span class="m">1</span>,3,5-9<span class="o">]</span></span>
<span class="prompt2">batch     up     infinite     <span class="m">6</span> idle   node<span class="o">[</span><span class="m">10</span>-15<span class="o">]</span></span>
<span class="prompt2">cpu       up     infinite     <span class="m">6</span> idle   cpu_node<span class="o">[</span><span class="m">1</span>-15<span class="o">]</span></span>
<span class="prompt2">gpu       up     infinite     <span class="m">6</span> idle   gpu_node<span class="o">[</span><span class="m">1</span>-15<span class="o">]</span></span>
</pre></div></div><p>To display available <em>nodes</em> and their status, you can use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sinfo -N -l</span>
<span class="prompt2">NODELIST    NODES PARTITION STATE  CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON</span>
<span class="prompt2">node<span class="o">[</span><span class="m">1</span>,3,5-9<span class="o">]</span>   <span class="m">2</span> batch     allocated <span class="m">2</span>    <span class="m">246</span>    <span class="m">16000</span>     <span class="m">0</span>  <span class="o">(</span>null<span class="o">)</span>   <span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">node<span class="o">[</span><span class="m">2</span>,4<span class="o">]</span>       <span class="m">2</span> batch     drain     <span class="m">2</span>    <span class="m">246</span>    <span class="m">16000</span>     <span class="m">0</span>  <span class="o">(</span>null<span class="o">)</span>   <span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">node<span class="o">[</span><span class="m">10</span>-15<span class="o">]</span>     <span class="m">6</span> batch     idle      <span class="m">2</span>    <span class="m">246</span>    <span class="m">16000</span>     <span class="m">0</span>  <span class="o">(</span>null<span class="o">)</span>   <span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">...</span>
</pre></div></div><p>And to get statistics on a job running or terminated, use <code class="docutils literal notranslate"><span class="pre">sacct</span></code> with some of
the fields you want to display</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sacct --format<span class="o">=</span>User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,nodelist,workdir -u <span class="nv">$USER</span></span>
<span class="prompt2">     User        JobID    JobName  Partition      State  Timelimit               Start                 End    Elapsed   NNodes      NCPUS        NodeList              WorkDir</span>
<span class="prompt2">--------- ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- ---------- -------- ---------- --------------- --------------------</span>
<span class="prompt2">my_usern+ <span class="m">2398</span>         run_extra+      batch    RUNNING <span class="m">130</span>-05:00+ <span class="m">2019</span>-03-27T18:33:43             Unknown <span class="m">1</span>-01:07:54        <span class="m">1</span>         <span class="m">16</span> node9           /home/mila/my_usern+</span>
<span class="prompt2">my_usern+ <span class="m">2399</span>         run_extra+      batch    RUNNING <span class="m">130</span>-05:00+ <span class="m">2019</span>-03-26T08:51:38             Unknown <span class="m">2</span>-10:49:59        <span class="m">1</span>         <span class="m">16</span> node9           /home/mila/my_usern+</span>
</pre></div></div><p>Or to get the list of all your previous jobs, use the <code class="docutils literal notranslate"><span class="pre">--start=YYYY-MM-DD</span></code> flag. You can check <code class="docutils literal notranslate"><span class="pre">sacct(1)</span></code> for further information about additional time formats.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sacct -u <span class="nv">$USER</span> --start<span class="o">=</span><span class="m">2019</span>-01-01</span>
</pre></div></div><p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code> (<a class="reference external" href="https://slurm.schedmd.com/scontrol.html">ref.</a>) can be used to
provide specific information on a job (currently running or recently terminated)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scontrol show job <span class="m">43123</span></span>
<span class="prompt2"><span class="nv">JobId</span><span class="o">=</span><span class="m">43123</span> <span class="nv">JobName</span><span class="o">=</span>python_script.py</span>
<span class="prompt2"><span class="nv">UserId</span><span class="o">=</span>my_username<span class="o">(</span><span class="m">1500000111</span><span class="o">)</span> <span class="nv">GroupId</span><span class="o">=</span>student<span class="o">(</span><span class="m">1500000000</span><span class="o">)</span> <span class="nv">MCS_label</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">Priority</span><span class="o">=</span><span class="m">645895</span> <span class="nv">Nice</span><span class="o">=</span><span class="m">0</span> <span class="nv">Account</span><span class="o">=</span>my_username <span class="nv">QOS</span><span class="o">=</span>normal</span>
<span class="prompt2"><span class="nv">JobState</span><span class="o">=</span>RUNNING <span class="nv">Reason</span><span class="o">=</span>None <span class="nv">Dependency</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">Requeue</span><span class="o">=</span><span class="m">1</span> <span class="nv">Restarts</span><span class="o">=</span><span class="m">3</span> <span class="nv">BatchFlag</span><span class="o">=</span><span class="m">1</span> <span class="nv">Reboot</span><span class="o">=</span><span class="m">0</span> <span class="nv">ExitCode</span><span class="o">=</span><span class="m">0</span>:0</span>
<span class="prompt2"><span class="nv">RunTime</span><span class="o">=</span><span class="m">2</span>-10:41:57 <span class="nv">TimeLimit</span><span class="o">=</span><span class="m">130</span>-05:00:00 <span class="nv">TimeMin</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">SubmitTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:47:17 <span class="nv">EligibleTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">AccrueTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">StartTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:51:38 <span class="nv">EndTime</span><span class="o">=</span><span class="m">2019</span>-08-03T13:51:38 <span class="nv">Deadline</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">PreemptTime</span><span class="o">=</span>None <span class="nv">SuspendTime</span><span class="o">=</span>None <span class="nv">SecsPreSuspend</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">LastSchedEval</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">Partition</span><span class="o">=</span>slurm_partition AllocNode:Sid<span class="o">=</span>login-node-1:14586</span>
<span class="prompt2"><span class="nv">ReqNodeList</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">ExcNodeList</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">NodeList</span><span class="o">=</span>node2</span>
<span class="prompt2"><span class="nv">BatchHost</span><span class="o">=</span>node2</span>
<span class="prompt2"><span class="nv">NumNodes</span><span class="o">=</span><span class="m">1</span> <span class="nv">NumCPUs</span><span class="o">=</span><span class="m">16</span> <span class="nv">NumTasks</span><span class="o">=</span><span class="m">1</span> CPUs/Task<span class="o">=</span><span class="m">16</span> ReqB:S:C:T<span class="o">=</span><span class="m">0</span>:0:*:*</span>
<span class="prompt2"><span class="nv">TRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M,node<span class="o">=</span><span class="m">1</span>,billing<span class="o">=</span><span class="m">3</span></span>
<span class="prompt2">Socks/Node<span class="o">=</span>* NtasksPerN:B:S:C<span class="o">=</span><span class="m">1</span>:0:*:* <span class="nv">CoreSpec</span><span class="o">=</span>*</span>
<span class="prompt2"><span class="nv">MinCPUsNode</span><span class="o">=</span><span class="m">16</span> <span class="nv">MinMemoryNode</span><span class="o">=</span>32000M <span class="nv">MinTmpDiskNode</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">Features</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">DelayBoot</span><span class="o">=</span><span class="m">00</span>:00:00</span>
<span class="prompt2"><span class="nv">OverSubscribe</span><span class="o">=</span>OK <span class="nv">Contiguous</span><span class="o">=</span><span class="m">0</span> <span class="nv">Licenses</span><span class="o">=(</span>null<span class="o">)</span> <span class="nv">Network</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">WorkDir</span><span class="o">=</span>/home/mila/my_username</span>
<span class="prompt2"><span class="nv">StdErr</span><span class="o">=</span>/home/mila/my_username/slurm-43123.out</span>
<span class="prompt2"><span class="nv">StdIn</span><span class="o">=</span>/dev/null</span>
<span class="prompt2"><span class="nv">StdOut</span><span class="o">=</span>/home/mila/my_username/slurm-43123.out</span>
<span class="prompt2"><span class="nv">Power</span><span class="o">=</span></span>
</pre></div></div><p>Or more info on a node and its resources</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scontrol show node node9</span>
<span class="prompt2"><span class="nv">NodeName</span><span class="o">=</span>node9 <span class="nv">Arch</span><span class="o">=</span>x86_64 <span class="nv">CoresPerSocket</span><span class="o">=</span><span class="m">4</span></span>
<span class="prompt2"><span class="nv">CPUAlloc</span><span class="o">=</span><span class="m">16</span> <span class="nv">CPUTot</span><span class="o">=</span><span class="m">16</span> <span class="nv">CPULoad</span><span class="o">=</span><span class="m">1</span>.38</span>
<span class="prompt2"><span class="nv">AvailableFeatures</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">ActiveFeatures</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">Gres</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">NodeAddr</span><span class="o">=</span><span class="m">10</span>.252.232.4 <span class="nv">NodeHostName</span><span class="o">=</span>mila20684000000 <span class="nv">Port</span><span class="o">=</span><span class="m">0</span> <span class="nv">Version</span><span class="o">=</span><span class="m">18</span>.08</span>
<span class="prompt2"><span class="nv">OS</span><span class="o">=</span>Linux <span class="m">4</span>.15.0-1036 <span class="c1">#38-Ubuntu SMP Fri Dec 7 02:47:47 UTC 2018</span></span>
<span class="prompt2"><span class="nv">RealMemory</span><span class="o">=</span><span class="m">32000</span> <span class="nv">AllocMem</span><span class="o">=</span><span class="m">32000</span> <span class="nv">FreeMem</span><span class="o">=</span><span class="m">23262</span> <span class="nv">Sockets</span><span class="o">=</span><span class="m">2</span> <span class="nv">Boards</span><span class="o">=</span><span class="m">1</span></span>
<span class="prompt2"><span class="nv">State</span><span class="o">=</span>ALLOCATED+CLOUD <span class="nv">ThreadsPerCore</span><span class="o">=</span><span class="m">2</span> <span class="nv">TmpDisk</span><span class="o">=</span><span class="m">0</span> <span class="nv">Weight</span><span class="o">=</span><span class="m">1</span> <span class="nv">Owner</span><span class="o">=</span>N/A <span class="nv">MCS_label</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">Partitions</span><span class="o">=</span>slurm_partition</span>
<span class="prompt2"><span class="nv">BootTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:50:01 <span class="nv">SlurmdStartTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:51:15</span>
<span class="prompt2"><span class="nv">CfgTRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M,billing<span class="o">=</span><span class="m">3</span></span>
<span class="prompt2"><span class="nv">AllocTRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M</span>
<span class="prompt2"><span class="nv">CapWatts</span><span class="o">=</span>n/a</span>
<span class="prompt2"><span class="nv">CurrentWatts</span><span class="o">=</span><span class="m">0</span> <span class="nv">LowestJoules</span><span class="o">=</span><span class="m">0</span> <span class="nv">ConsumedJoules</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">ExtSensorsJoules</span><span class="o">=</span>n/s <span class="nv">ExtSensorsWatts</span><span class="o">=</span><span class="m">0</span> <span class="nv">ExtSensorsTemp</span><span class="o">=</span>n/s</span>
</pre></div></div></section>
</section>
<section id="useful-commands">
<h3>Useful Commands<a class="headerlink" href="#useful-commands" title="Permalink to this heading"></a></h3>
<dl class="simple">
<dt>salloc</dt><dd><p>Get an interactive job and give you a shell. (ssh like) CPU only</p>
</dd>
<dt>salloc --gres=gpu:1 -c 2 --mem=12000</dt><dd><p>Get an interactive job with one GPU, 2 CPUs and 12000 MB RAM</p>
</dd>
<dt>sbatch</dt><dd><p>start a batch job (same options as salloc)</p>
</dd>
<dt>sattach --pty &lt;jobid&gt;.0</dt><dd><p>Re-attach a dropped interactive job</p>
</dd>
<dt>sinfo</dt><dd><p>status of all nodes</p>
</dd>
<dt>sinfo -Ogres:27,nodelist,features -tidle,mix,alloc</dt><dd><p>List GPU type and FEATURES that you can request</p>
</dd>
<dt>savail</dt><dd><p>(Custom) List available gpu</p>
</dd>
<dt>scancel &lt;jobid&gt;</dt><dd><p>Cancel a job</p>
</dd>
<dt>squeue</dt><dd><p>summary status of all active jobs</p>
</dd>
<dt>squeue -u $USER</dt><dd><p>summary status of all YOUR active jobs</p>
</dd>
<dt>squeue -j &lt;jobid&gt;</dt><dd><p>summary status of a specific job</p>
</dd>
<dt>squeue -Ojobid,name,username,partition,state,timeused,nodelist,gres,tres</dt><dd><p>status of all jobs including requested resources (see the SLURM squeue doc for all output options)</p>
</dd>
<dt>scontrol show job &lt;jobid&gt;</dt><dd><p>Detailed status of a running job</p>
</dd>
<dt>sacct -j &lt;job_id&gt; -o NodeList</dt><dd><p>Get the node where a finished job ran</p>
</dd>
<dt>sacct -u $USER -S &lt;start_time&gt; -E &lt;stop_time&gt;</dt><dd><p>Find info about old jobs</p>
</dd>
<dt>sacct -oJobID,JobName,User,Partition,Node,State</dt><dd><p>List of current and recent jobs</p>
</dd>
</dl>
</section>
<section id="special-gpu-requirements">
<h3>Special GPU requirements<a class="headerlink" href="#special-gpu-requirements" title="Permalink to this heading"></a></h3>
<p>Specific GPU <em>architecture</em> and <em>memory</em> can be easily requested through the
<code class="docutils literal notranslate"><span class="pre">--gres</span></code> flag by using either</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:architecture:memory:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:architecture:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:memory:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:model:number</span></code></p></li>
</ul>
<p><em>Example:</em></p>
<p>To request a Tesla GPU with <em>at least</em> 16GB of memory use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch -c <span class="m">4</span> --gres<span class="o">=</span>gpu:tesla:16gb:1</span>
</pre></div></div><p>The full list of GPU and their features can be accessed <a class="reference internal" href="Information.html#node-list"><span class="std std-ref">here</span></a>.</p>
</section>
<section id="cpu-only-jobs">
<h3>CPU-only jobs<a class="headerlink" href="#cpu-only-jobs" title="Permalink to this heading"></a></h3>
<p>Since the priority is given to the usage of GPUs, CPU-only jobs have a low
priority and can only consume <strong>4 cpus maximum per node</strong>.  The partition for
CPU-only jobs is named <code class="docutils literal notranslate"><span class="pre">cpu_jobs</span></code> and you can request it with <code class="docutils literal notranslate"><span class="pre">-p</span> <span class="pre">cpu_jobs</span></code>
or if you don’t specify any GPU, you will be automatically rerouted to this
partition.</p>
</section>
<section id="example-script">
<h3>Example script<a class="headerlink" href="#example-script" title="Permalink to this heading"></a></h3>
<p>Here is a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script that follows good practices on the Mila cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --partition=unkillable                           # Ask for unkillable job</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --cpus-per-task=2                                # Ask for 2 CPUs</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:1                                     # Ask for 1 GPU</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --mem=10G                                        # Ask for 10 GB of RAM</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --time=3:00:00                                   # The job will run for 3 hours</span>
<span class="linenos"> 8</span><span class="c1">#SBATCH -o /network/scratch/&lt;u&gt;/&lt;username&gt;/slurm-%j.out  # Write the log on scratch</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># 1. Load the required modules</span>
<span class="linenos">11</span>module --quiet load anaconda/3
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># 2. Load your environment</span>
<span class="linenos">14</span>conda activate <span class="s2">&quot;&lt;env_name&gt;&quot;</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1"># 3. Copy your dataset on the compute node</span>
<span class="linenos">17</span>cp /network/datasets/&lt;dataset&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># 4. Launch your job, tell it to save the model in $SLURM_TMPDIR</span>
<span class="linenos">20</span><span class="c1">#    and look for the dataset into $SLURM_TMPDIR</span>
<span class="linenos">21</span>python main.py --path <span class="nv">$SLURM_TMPDIR</span> --data_path <span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="c1"># 5. Copy whatever you want to save on $SCRATCH</span>
<span class="linenos">24</span>cp <span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt; /network/scratch/&lt;u&gt;/&lt;username&gt;/
</pre></div>
</div>
</section>
</section>
<section id="portability-concerns-and-solutions">
<h2>Portability concerns and solutions<a class="headerlink" href="#portability-concerns-and-solutions" title="Permalink to this heading"></a></h2>
<p>When working on a software project, it is important to be aware of all the
software and libraries the project relies on and to list them explicitly and
<em>under a version control system</em> in such a way that they can easily be
installed and made available on different systems. The upsides are significant:</p>
<ul class="simple">
<li><p>Easily install and run on the cluster</p></li>
<li><p>Ease of collaboration</p></li>
<li><p>Better reproducibility</p></li>
</ul>
<p>To achieve this, try to always keep in mind the following aspects:</p>
<ul class="simple">
<li><p><strong>Versions:</strong> For each dependency, make sure you have some record of the
specific version you are using during development. That way, in the future, you
will be able to reproduce the original environment which you know to be
compatible. Indeed, the more time passes, the more likely it is that newer
versions of some dependency have breaking changes. The <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">freeze</span></code> command can create
such a record for Python dependencies.</p></li>
<li><p><strong>Isolation:</strong> Ideally, each of your software projects should be isolated from
the others. What this means is that updating the environment for project A
should <em>not</em> update the environment for project B. That way, you can freely
install and upgrade software and libraries for the former without worrying about
breaking the latter (which you might not notice until weeks later, the next time
you work on project B!) Isolation can be achieved using <a class="reference internal" href="Theory_cluster.html#python-virtual-environments"><span class="std std-ref">Python Virtual environments</span></a> and <a class="reference internal" href="Theory_cluster.html#containers"><span class="std std-ref">Containers</span></a>.</p></li>
</ul>
<section id="managing-your-environments">
<h3>Managing your environments<a class="headerlink" href="#managing-your-environments" title="Permalink to this heading"></a></h3>
</section>
<section id="virtual-environments">
<span id="python"></span><h3>Virtual environments<a class="headerlink" href="#virtual-environments" title="Permalink to this heading"></a></h3>
<p>A virtual environment in Python is a local, isolated environment in which you
can install or uninstall Python packages without interfering with the global
environment (or other virtual environments). It usually lives in a directory
(location varies depending on whether you use venv, conda or poetry). In order
to use a virtual environment, you have to <strong>activate</strong> it. Activating an
environment essentially sets environment variables in your shell so that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python</span></code> points to the right Python version for that environment (different
virtual environments can use different versions of Python!)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span></code> looks for packages in the virtual environment</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> installs packages into the virtual environment</p></li>
<li><p>Any shell commands installed via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> are made available</p></li>
</ul>
<p>To run experiments within a virtual environment, you can simply activate it
in the script given to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<section id="pip-virtualenv">
<h4>Pip/Virtualenv<a class="headerlink" href="#pip-virtualenv" title="Permalink to this heading"></a></h4>
<p>Pip is the preferred package manager for Python and each cluster provides
several Python versions through the associated module which comes with pip. In
order to install new packages, you will first have to create a personal space
for them to be stored.  The preferred solution (as it is the preferred solution
on Compute Canada clusters) is to use <a class="reference external" href="https://virtualenv.pypa.io/en/stable/">virtual environments</a>.</p>
<p>First, load the Python module you want to use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load python/3.9</span>
</pre></div></div><p>Then, create a virtual environment in your <code class="docutils literal notranslate"><span class="pre">home</span></code> directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">virtualenv <span class="nv">$HOME</span>/&lt;env&gt;</span>
</pre></div></div><p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. Finally, activate the environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">source</span> <span class="nv">$HOME</span>/&lt;env&gt;/bin/activate</span>
</pre></div></div><p>You can now install any Python package you wish using the <code class="docutils literal notranslate"><span class="pre">pip</span></code> command, e.g.
<a class="reference external" href="https://pytorch.org/get-started/locally">pytorch</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt3:before {
  content: "(<env>)$ ";
}
</style><span class="prompt3">pip install torch torchvision</span>
</pre></div></div><p>Or <a class="reference external" href="https://www.tensorflow.org/install/gpu">Tensorflow</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">pip install tensorflow-gpu</span>
</pre></div></div></section>
<section id="conda">
<h4>Conda<a class="headerlink" href="#conda" title="Permalink to this heading"></a></h4>
<p>Another solution for Python is to use <a class="reference external" href="https://docs.conda.io/en/latest/miniconda.html">miniconda</a> or <a class="reference external" href="https://docs.anaconda.com">anaconda</a> which are also available through the <code class="docutils literal notranslate"><span class="pre">module</span></code>
command: (the use of Conda is not recommended for Compute Canada Clusters due to
the availability of custom-built packages for pip)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load miniconda/3</span>
<span class="prompt2"><span class="o">===</span> Module miniconda/3 <span class="nv">loaded</span> <span class="o">===]</span></span>
<span class="prompt2">o <span class="nb">enable</span> conda environment functions, first use:</span>
</pre></div></div><p>To create an environment (see <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">here</a>
for details) using a specific Python version, you may write:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda create -n &lt;env&gt; <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9</span>
</pre></div></div><p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. You can now activate it by doing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda activate &lt;env&gt;</span>
</pre></div></div><p>You are now ready to install any Python package you want in this environment.
For instance, to install PyTorch, you can find the Conda command of any version
you want on <a class="reference external" href="https://pytorch.org/get-started/locally">pytorch’s website</a>, e.g:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">conda install pytorch torchvision <span class="nv">cudatoolkit</span><span class="o">=</span><span class="m">10</span>.0 -c pytorch</span>
</pre></div></div><p>If you make a lot of environments and install/uninstall a lot of packages, it
can be good to periodically clean up Conda’s cache:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">conda clean --all</span>
</pre></div></div></section>
</section>
<section id="using-modules">
<h3>Using Modules<a class="headerlink" href="#using-modules" title="Permalink to this heading"></a></h3>
<p>A lot of software, such as Python and Conda, is already compiled and available on
the cluster through the <code class="docutils literal notranslate"><span class="pre">module</span></code> command and its sub-commands. In particular,
if you wish to use <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">3.7</span></code> you can simply do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load python/3.7</span>
</pre></div></div><section id="the-module-command">
<h4>The module command<a class="headerlink" href="#the-module-command" title="Permalink to this heading"></a></h4>
<p>For a list of available modules, simply use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module avail</span>
<span class="prompt2">-------------------------------------------------------------------------------------------------------------- Global Aliases ---------------------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  cuda/10.0 -&gt; cudatoolkit/10.0    cuda/9.2      -&gt; cudatoolkit/9.2                                 pytorch/1.4.1       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1    tensorflow/1.15 -&gt; python/3.7/tensorflow/1.15</span>
<span class="prompt2">  cuda/10.1 -&gt; cudatoolkit/10.1    mujoco-py     -&gt; python/3.7/mujoco-py/2.0                        pytorch/1.5.0       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0    tensorflow/2.2  -&gt; python/3.7/tensorflow/2.2</span>
<span class="prompt2">  cuda/10.2 -&gt; cudatoolkit/10.2    mujoco-py/2.0 -&gt; python/3.7/mujoco-py/2.0                        pytorch/1.5.1       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1</span>
<span class="prompt2">  cuda/11.0 -&gt; cudatoolkit/11.0    pytorch       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1    tensorflow          -&gt; python/3.7/tensorflow/2.2</span>
<span class="prompt2">  cuda/9.0  -&gt; cudatoolkit/9.0     pytorch/1.4.0 -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.0    tensorflow-cpu/1.15 -&gt; python/3.7/tensorflow/1.15</span>
<span class="prompt2"></span>
<span class="prompt2">-------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Core ---------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  Mila       <span class="o">(</span>S,L<span class="o">)</span>    anaconda/3 <span class="o">(</span>D<span class="o">)</span>    go/1.13.5        miniconda/2        mujoco/1.50        python/2.7    python/3.6        python/3.8           singularity/3.0.3    singularity/3.2.1    singularity/3.5.3 <span class="o">(</span>D<span class="o">)</span></span>
<span class="prompt2">  anaconda/2          go/1.12.4         go/1.14   <span class="o">(</span>D<span class="o">)</span>    miniconda/3 <span class="o">(</span>D<span class="o">)</span>    mujoco/2.0  <span class="o">(</span>D<span class="o">)</span>    python/3.5    python/3.7 <span class="o">(</span>D<span class="o">)</span>    singularity/2.6.1    singularity/3.1.1    singularity/3.4.2</span>
<span class="prompt2"></span>
<span class="prompt2">------------------------------------------------------------------------------------------------ /cvmfs/config.mila.quebec/modules/Compiler -------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  python/3.7/mujoco-py/2.0</span>
<span class="prompt2"></span>
<span class="prompt2">-------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Cuda ---------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  cuda/10.0/cudnn/7.3        cuda/10.0/nccl/2.4         cuda/10.1/nccl/2.4     cuda/11.0/nccl/2.7        cuda/9.0/nccl/2.4     cudatoolkit/9.0     cudatoolkit/10.1        cudnn/7.6/cuda/10.0/tensorrt/7.0</span>
<span class="prompt2">  cuda/10.0/cudnn/7.5        cuda/10.1/cudnn/7.5        cuda/10.2/cudnn/7.6    cuda/9.0/cudnn/7.3        cuda/9.2/cudnn/7.6    cudatoolkit/9.2     cudatoolkit/10.2        cudnn/7.6/cuda/10.1/tensorrt/7.0</span>
<span class="prompt2">  cuda/10.0/cudnn/7.6 <span class="o">(</span>D<span class="o">)</span>    cuda/10.1/cudnn/7.6 <span class="o">(</span>D<span class="o">)</span>    cuda/10.2/nccl/2.7     cuda/9.0/cudnn/7.5 <span class="o">(</span>D<span class="o">)</span>    cuda/9.2/nccl/2.4     cudatoolkit/10.0    cudatoolkit/11.0 <span class="o">(</span>D<span class="o">)</span>    cudnn/7.6/cuda/9.0/tensorrt/7.0</span>
<span class="prompt2"></span>
<span class="prompt2">------------------------------------------------------------------------------------------------ /cvmfs/config.mila.quebec/modules/Pytorch --------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.4.1    python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.1 <span class="o">(</span>D<span class="o">)</span>    python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0</span>
<span class="prompt2">  python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.0    python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1        python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1 <span class="o">(</span>D<span class="o">)</span></span>
<span class="prompt2"></span>
<span class="prompt2">----------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Tensorflow ------------------------------------------------------------------------------------------------</span>
<span class="prompt2">  python/3.7/tensorflow/1.15    python/3.7/tensorflow/2.0    python/3.7/tensorflow/2.2 <span class="o">(</span>D<span class="o">)</span></span>
</pre></div></div><p>Modules can be loaded using the <code class="docutils literal notranslate"><span class="pre">load</span></code> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load &lt;module&gt;</span>
</pre></div></div><p>To search for a module or a software, use the command <code class="docutils literal notranslate"><span class="pre">spider</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module spider search_term</span>
</pre></div></div><p>E.g.: by default, <code class="docutils literal notranslate"><span class="pre">python2</span></code> will refer to the os-shipped installation of <code class="docutils literal notranslate"><span class="pre">python2.7</span></code> and <code class="docutils literal notranslate"><span class="pre">python3</span></code> to <code class="docutils literal notranslate"><span class="pre">python3.6</span></code>.
If you want to use <code class="docutils literal notranslate"><span class="pre">python3.7</span></code> you can type:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load python3.7</span>
</pre></div></div></section>
<section id="available-software">
<h4>Available Software<a class="headerlink" href="#available-software" title="Permalink to this heading"></a></h4>
<p>Modules are divided in 5 main sections:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Section</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Core</p></td>
<td><p>Base interpreter and software (Python, go, etc…)</p></td>
</tr>
<tr class="row-odd"><td><p>Compiler</p></td>
<td><p>Interpreter-dependent software (<em>see the note below</em>)</p></td>
</tr>
<tr class="row-even"><td><p>Cuda</p></td>
<td><p>Toolkits, cudnn and related libraries</p></td>
</tr>
<tr class="row-odd"><td><p>Pytorch/Tensorflow</p></td>
<td><p>Pytorch/TF built with a specific Cuda/Cudnn
version for Mila’s GPUs (<em>see the related paragraph</em>)</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Modules which are nested (../../..) usually depend on other software/module
loaded alongside the main module.  No need to load the dependent software,
the complex naming scheme allows an automatic detection of the dependent
module(s):</p>
<p>i.e.: Loading <code class="docutils literal notranslate"><span class="pre">cudnn/7.6/cuda/9.0/tensorrt/7.0</span></code> will load <code class="docutils literal notranslate"><span class="pre">cudnn/7.6</span></code> and
<code class="docutils literal notranslate"><span class="pre">cuda/9.0</span></code> alongside</p>
<p><code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> is a particular dependency which can be served through
<code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> or <code class="docutils literal notranslate"><span class="pre">anaconda/3</span></code> and is not automatically loaded to let the
user pick his favorite flavor.</p>
</div>
</section>
<section id="default-package-location">
<h4>Default package location<a class="headerlink" href="#default-package-location" title="Permalink to this heading"></a></h4>
<p>Python by default uses the user site package first and packages provided by
<code class="docutils literal notranslate"><span class="pre">module</span></code> last to not interfere with your installation.  If you want to skip
packages installed in your site-packages folder (in your /home directory), you
have to start Python with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag.</p>
<p>To check which package is loaded at import, you can print <code class="docutils literal notranslate"><span class="pre">package.__file__</span></code>
to get the full path of the package.</p>
<p><em>Example:</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load pytorch/1.5.0</span>
<span class="prompt1">python -c <span class="s1">&#39;import torch;print(torch.__file__)&#39;</span></span>
<span class="prompt2">home/mila/my_home/.local/lib/python3.7/site-packages/torch/__init__.py   &lt;<span class="o">==</span> package from your own site-package</span>
</pre></div></div><p>Now with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module load pytorch/1.5.0</span>
<span class="prompt1">python -s -c <span class="s1">&#39;import torch;print(torch.__file__)&#39;</span></span>
<span class="prompt2">cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch/python3.7-cuda10.1-cudnn7.6-v1.5.0/lib/python3.7/site-packages/torch/__init__.py<span class="err">&#39;</span></span>
</pre></div></div></section>
</section>
<section id="on-using-containers">
<h3>On using containers<a class="headerlink" href="#on-using-containers" title="Permalink to this heading"></a></h3>
<p>Another option for creating portable code is <a class="reference internal" href="#using-containers"><span class="std std-ref">Using containers on clusters</span></a>.</p>
<p>Containers are a popular approach at deploying applications by packaging a lot
of the required dependencies together. The most popular tool for this is
<a class="reference external" href="https://www.docker.com/">Docker</a>, but Docker cannot be used on the Mila
cluster (nor the other clusters from Compute Canada).</p>
<p>One popular mechanism for containerisation on a computational cluster is called
<a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity</a>.
This is the recommended approach for running containers on the
Mila cluster. See section <a class="reference internal" href="#id2"><span class="std std-ref">Singularity</span></a> for more details.</p>
</section>
</section>
<section id="id2">
<h2>Singularity<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h3>
<section id="what-is-singularity">
<h4>What is Singularity?<a class="headerlink" href="#what-is-singularity" title="Permalink to this heading"></a></h4>
<p>Running Docker on SLURM is a security problem (e.g. running as root, being able
to mount any directory).  The alternative is to use Singularity, which is a
popular solution in the world of HPC.</p>
<p>There is a good level of compatibility between Docker and Singularity,
and we can find many exaggerated claims about able to convert containers
from Docker to Singularity without any friction.
Oftentimes, Docker images from DockerHub are 100% compatible with Singularity,
and they can indeed be used without friction, but things get messy when
we try to convert our own Docker build files to Singularity recipes.</p>
</section>
<section id="links-to-official-documentation">
<h4>Links to official documentation<a class="headerlink" href="#links-to-official-documentation" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>official <a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity user guide</a> (this is the one you
will use most often)</p></li>
<li><p>official <a class="reference external" href="https://sylabs.io/guides/latest/admin-guide/">Singularity admin guide</a></p></li>
</ul>
</section>
<section id="overview-of-the-steps-used-in-practice">
<h4>Overview of the steps used in practice<a class="headerlink" href="#overview-of-the-steps-used-in-practice" title="Permalink to this heading"></a></h4>
<p>Most often, the process to create and use a Singularity container is:</p>
<ul class="simple">
<li><p>on your Linux computer (at home or work)</p>
<ul>
<li><p>select a Docker image from DockerHub (e.g. <em>pytorch/pytorch</em>)</p></li>
<li><p>make a recipe file for Singularity that starts with that DockerHub image</p></li>
<li><p>build the recipe file, thus creating the image file (e.g. <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>)</p></li>
<li><p>test your singularity container before send it over to the cluster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span> <span class="pre">-av</span> <span class="pre">my-pytorch-image.sif</span> <span class="pre">&lt;login-node&gt;:Documents/my-singularity-images</span></code></p></li>
</ul>
</li>
<li><p>on the login node for that cluster</p>
<ul>
<li><p>queue your jobs with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">...</span></code></p></li>
<li><p>(note that your jobs will copy over the <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code> to $SLURM_TMPDIR
and will then launch Singularity with that image)</p></li>
<li><p>do something else while you wait for them to finish</p></li>
<li><p>queue more jobs with the same <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>,
reusing it many times over</p></li>
</ul>
</li>
</ul>
<p>In the following sections you will find specific examples or tips to accomplish
in practice the steps highlighted above.</p>
</section>
<section id="nope-not-on-macos">
<h4>Nope, not on MacOS<a class="headerlink" href="#nope-not-on-macos" title="Permalink to this heading"></a></h4>
<p>Singularity does not work on MacOS, as of the time of this writing in 2021.
Docker does not <em>actually</em> run on MacOS, but there Docker silently installs a
virtual machine running Linux, which makes it a pleasant experience,
and the user does not need to care about the details of how Docker does it.</p>
<p>Given its origins in HPC, Singularity does not provide that kind of seamless
experience on MacOS, even though it’s technically possible to run it
inside a Linux virtual machine on MacOS.</p>
</section>
<section id="where-to-build-images">
<h4>Where to build images<a class="headerlink" href="#where-to-build-images" title="Permalink to this heading"></a></h4>
<p>Building Singularity images is a rather heavy task, which can take 20 minutes
if you have a lot of steps in your recipe. This makes it a bad task to run on
the login nodes of our clusters, especially if it needs to be run regularly.</p>
<p>On the Mila cluster, we are lucky to have unrestricted internet access on the compute
nodes, which means that anyone can request an interactive CPU node (no need for GPU)
and build their images there without problem.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not build Singularity images from scratch every time your run a
job in a large batch.  This will be a colossal waste of GPU time as well as
internet bandwidth.  If you setup your workflow properly (e.g. using bind
paths for your code and data), you can spend months reusing the same
Singularity image <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>.</p>
</div>
</section>
</section>
<section id="building-the-containers">
<h3>Building the containers<a class="headerlink" href="#building-the-containers" title="Permalink to this heading"></a></h3>
<p>Building a container is like creating a new environment except that containers
are much more powerful since they are self-contained systems.  With
singularity, there are two ways to build containers.</p>
<p>The first one is by yourself, it’s like when you got a new Linux laptop and you
don’t really know what you need, if you see that something is missing, you
install it. Here you can get a vanilla container with Ubuntu called a sandbox,
you log in and you install each packages by yourself.  This procedure can take
time but will allow you to understand how things work and what you need. This is
recommended if you need to figure out how things will be compiled or if you want
to install packages on the fly. We’ll refer to this procedure as singularity
sandboxes.</p>
<p>The second way is more like you know what you want, so you write a list of
everything you need, you send it to singularity and it will install everything
for you. Those lists are called singularity recipes.</p>
<section id="first-way-build-and-use-a-sandbox">
<h4>First way: Build and use a sandbox<a class="headerlink" href="#first-way-build-and-use-a-sandbox" title="Permalink to this heading"></a></h4>
<p>You might ask yourself: <em>On which machine should I build a container?</em></p>
<p>First of all, you need to choose where you’ll build your container. This
operation requires <strong>memory and high cpu usage</strong>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do NOT build containers on any login nodes !</p>
</div>
<ul>
<li><p>(Recommended for beginner) If you need to <strong>use apt-get</strong>, you should <strong>build
the container on your laptop</strong> with sudo privileges. You’ll only need to
install singularity on your laptop. Windows/Mac users can look <a class="reference external" href="https://www.sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac">there</a> and
Ubuntu/Debian users can use directly:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sudo apt-get install singularity-container</span>
</pre></div></div></div></blockquote>
</li>
<li><p>If you <strong>can’t install singularity</strong> on your laptop and you <strong>don’t need
apt-get</strong>, you can reserve a <strong>cpu node on the Mila cluster</strong> to build your
container.</p></li>
</ul>
<p>In this case, in order to avoid too much I/O over the network, you should define
the singularity cache locally:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">export</span> <span class="nv">SINGULARITY_CACHEDIR</span><span class="o">=</span><span class="nv">$SLURM_TMPDIR</span></span>
</pre></div></div></div></blockquote>
<ul class="simple">
<li><p>If you <strong>can’t install singularity</strong> on your laptop and you <strong>want to use
apt-get</strong>, you can use <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> to build your containers and read
<a class="reference internal" href="#recipe-section">Recipe_section</a>.</p></li>
</ul>
<section id="download-containers-from-the-web">
<h5>Download containers from the web<a class="headerlink" href="#download-containers-from-the-web" title="Permalink to this heading"></a></h5>
<p>Hopefully, you may not need to create containers from scratch as many have been
already built for the most common deep learning software. You can find most of
them on <a class="reference external" href="https://hub.docker.com/">dockerhub</a>.</p>
<p>Go on <a class="reference external" href="https://hub.docker.com/">dockerhub</a> and select the container you want to pull.</p>
<p>For example, if you want to get the latest PyTorch version with GPU support
(Replace <em>runtime</em> by <em>devel</em> if you need the full Cuda toolkit):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity pull docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime</span>
</pre></div></div><p>Or the latest TensorFlow:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity pull docker://tensorflow/tensorflow:latest-gpu-py3</span>
</pre></div></div><p>Currently the pulled image <code class="docutils literal notranslate"><span class="pre">pytorch.simg</span></code> or <code class="docutils literal notranslate"><span class="pre">tensorflow.simg</span></code> is read-only
meaning that you won’t be able to install anything on it.  Starting now, PyTorch
will be taken as example. If you use TensorFlow, simply replace every
<strong>pytorch</strong> occurrences by <strong>tensorflow</strong>.</p>
</section>
<section id="how-to-add-or-install-stuff-in-a-container">
<h5>How to add or install stuff in a container<a class="headerlink" href="#how-to-add-or-install-stuff-in-a-container" title="Permalink to this heading"></a></h5>
<p>The first step is to transform your read only container
<code class="docutils literal notranslate"><span class="pre">pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span></code> in a writable version that will
allow you to add packages.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Depending on the version of singularity you are using, singularity
will build a container with the extension .simg or .sif. If you’re using
.sif files, replace every occurences of .simg by .sif.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to use <strong>apt-get</strong> you have to put <strong>sudo</strong> ahead of the
following commands</p>
</div>
<p>This command will create a writable image in the folder <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity build --sandbox pytorch pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span>
</pre></div></div><p>Then you’ll need the following command to log inside the container.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity shell --writable -H <span class="nv">$HOME</span>:/home pytorch</span>
</pre></div></div><p>Once you get into the container, you can use pip and install anything you need
(Or with <code class="docutils literal notranslate"><span class="pre">apt-get</span></code> if you built the container with sudo).</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Singularity mounts your home folder, so if you install things into
the <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> of your container, they will be installed in your real
<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>!</p>
</div>
<p>You should install your stuff in /usr/local instead.</p>
</section>
<section id="creating-useful-directories">
<h5>Creating useful directories<a class="headerlink" href="#creating-useful-directories" title="Permalink to this heading"></a></h5>
<p>One of the benefits of containers is that you’ll be able to use them across
different clusters. However for each cluster the datasets and experiments
folder location can be different. In order to be invariant to those locations,
we will create some useful mount points inside the container:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt4:before {
  content: "<Singularity_container>$ ";
}
</style><span class="prompt4">mkdir /dataset</span>
<span class="prompt4">mkdir /tmp_log</span>
<span class="prompt4">mkdir /final_log</span>
</pre></div></div><p>From now, you won’t need to worry anymore when you write your code to specify
where to pick up your dataset. Your dataset will always be in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>
independently of the cluster you are using.</p>
</section>
<section id="testing">
<h5>Testing<a class="headerlink" href="#testing" title="Permalink to this heading"></a></h5>
<p>If you have some code that you want to test before finalizing your container,
you have two choices.  You can either log into your container and run Python
code inside it with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity shell --nv pytorch</span>
</pre></div></div><p>Or you can execute your command directly with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity <span class="nb">exec</span> --nv pytorch Python YOUR_CODE.py</span>
</pre></div></div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>—nv allows the container to use gpus. You don’t need this if you
don’t plan to use a gpu.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Don’t forget to clear the cache of the packages you installed in
the containers.</p>
</div>
</section>
<section id="creating-a-new-image-from-the-sandbox">
<h5>Creating a new image from the sandbox<a class="headerlink" href="#creating-a-new-image-from-the-sandbox" title="Permalink to this heading"></a></h5>
<p>Once everything you need is installed inside the container, you need to convert
it back to a read-only singularity image with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity build pytorch_final.simg pytorch</span>
</pre></div></div></section>
</section>
<section id="second-way-use-recipes">
<span id="recipe-section"></span><h4>Second way: Use recipes<a class="headerlink" href="#second-way-use-recipes" title="Permalink to this heading"></a></h4>
<p>A singularity recipe is a file including specifics about installation software,
environment variables, files to add, and container metadata.  It is a starting
point for designing any custom container. Instead of pulling a container and
installing your packages manually, you can specify in this file the packages
you want and then build your container from this file.</p>
<p>Here is a toy example of a singularity recipe installing some stuff:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">################# Header: Define the base system you want to use ################</span>
<span class="c1"># Reference of the kind of base you want to use (e.g., docker, debootstrap, shub).</span>
Bootstrap: docker
<span class="c1"># Select the docker image you want to use (Here we choose tensorflow)</span>
From: tensorflow/tensorflow:latest-gpu-py3

<span class="c1">################# Section: Defining the system #################################</span>
<span class="c1"># Commands in the %post section are executed within the container.</span>
%post
        <span class="nb">echo</span> <span class="s2">&quot;Installing Tools with apt-get&quot;</span>
        apt-get update
        apt-get install -y cmake libcupti-dev libyaml-dev wget unzip
        apt-get clean
        <span class="nb">echo</span> <span class="s2">&quot;Installing things with pip&quot;</span>
        pip install tqdm
        <span class="nb">echo</span> <span class="s2">&quot;Creating mount points&quot;</span>
        mkdir /dataset
        mkdir /tmp_log
        mkdir /final_log


<span class="c1"># Environment variables that should be sourced at runtime.</span>
%environment
        <span class="c1"># use bash as default shell</span>
        <span class="nv">SHELL</span><span class="o">=</span>/bin/bash
        <span class="nb">export</span> SHELL
</pre></div>
</div>
<p>A recipe file contains two parts: the <code class="docutils literal notranslate"><span class="pre">header</span></code> and <code class="docutils literal notranslate"><span class="pre">sections</span></code>. In the
<code class="docutils literal notranslate"><span class="pre">header</span></code> you specify which base system you want to use, it can be any docker
or singularity container. In <code class="docutils literal notranslate"><span class="pre">sections</span></code>, you can list the things you want to
install in the subsection <code class="docutils literal notranslate"><span class="pre">post</span></code> or list the environment’s variable you need
to source at each runtime in the subsection <code class="docutils literal notranslate"><span class="pre">environment</span></code>. For a more detailed
description, please look at the <a class="reference external" href="https://www.sylabs.io/guides/2.6/user-guide/container_recipes.html#container-recipes">singularity documentation</a>.</p>
<p>In order to build a singularity container from a singularity recipe file, you
should use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sudo singularity build &lt;NAME_CONTAINER&gt; &lt;YOUR_RECIPE_FILES&gt;</span>
</pre></div></div><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You always need to use sudo when you build a container from a
recipe. As there is no access to sudo on the cluster, a personal computer or
the use singularity hub is needed to build a container</p>
</div>
<section id="build-recipe-on-singularity-hub">
<h5>Build recipe on singularity hub<a class="headerlink" href="#build-recipe-on-singularity-hub" title="Permalink to this heading"></a></h5>
<p>Singularity hub allows users to build containers from recipes directly on
singularity-hub’s cloud meaning that you don’t need to build containers by
yourself.  You need to register on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and link your
singularity-hub account to your GitHub account, then:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Create a new github repository.</p></li>
<li><p>Add a collection on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and select the github repository your created.</p></li>
<li><p>Clone the github repository on your computer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git clone &lt;url&gt;
</pre></div>
</div>
</li>
<li><p>Write the singularity recipe and save it as a file named <strong>Singularity</strong>.</p></li>
<li><p>Git add <strong>Singularity</strong>, commit and push on the master branch</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git add Singularity
$ git commit
$ git push origin master
</pre></div>
</div>
</li>
</ol>
</div></blockquote>
<p>At this point, robots from singularity-hub will build the container for you, you
will be able to download your container from the website or directly with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity pull shub://&lt;github_username&gt;/&lt;repository_name&gt;</span>
</pre></div></div></section>
<section id="example-recipe-with-openai-gym-mujoco-and-miniworld">
<h5>Example: Recipe with OpenAI gym, MuJoCo and Miniworld<a class="headerlink" href="#example-recipe-with-openai-gym-mujoco-and-miniworld" title="Permalink to this heading"></a></h5>
<p>Here is an example on how you can use a singularity recipe to install complex
environment such as OpenAI gym, MuJoCo and Miniworld on a PyTorch based
container. In order to use MuJoCo, you’ll need to copy the key stored on the
Mila cluster in <cite>/ai/apps/mujoco/license/mjkey.txt</cite> to your current directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a dockerfile that sets up a full Gym install with test dependencies</span>
Bootstrap: docker

<span class="c1"># Here we ll build our container upon the pytorch container</span>
From: pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime

<span class="c1"># Now we&#39;ll copy the mjkey file located in the current directory inside the container&#39;s root</span>
<span class="c1"># directory</span>
%files
        mjkey.txt

<span class="c1"># Then we put everything we need to install</span>
%post
        <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/opt/conda/bin
        apt -y update <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt install -y keyboard-configuration <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt install -y <span class="se">\</span>
        python3-dev <span class="se">\</span>
        python-pyglet <span class="se">\</span>
        python3-opengl <span class="se">\</span>
        libhdf5-dev <span class="se">\</span>
        libjpeg-dev <span class="se">\</span>
        libboost-all-dev <span class="se">\</span>
        libsdl2-dev <span class="se">\</span>
        libosmesa6-dev <span class="se">\</span>
        patchelf <span class="se">\</span>
        ffmpeg <span class="se">\</span>
        xvfb <span class="se">\</span>
        libhdf5-dev <span class="se">\</span>
        openjdk-8-jdk <span class="se">\</span>
        wget <span class="se">\</span>
        git <span class="se">\</span>
        unzip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt clean <span class="o">&amp;&amp;</span> <span class="se">\</span>
        rm -rf /var/lib/apt/lists/*
        pip install h5py

        <span class="c1"># Download Gym and MuJoCo</span>
        mkdir /Gym <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /Gym
        git clone https://github.com/openai/gym.git <span class="o">||</span> <span class="nb">true</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
        mkdir /Gym/.mujoco <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /Gym/.mujoco
        wget https://www.roboti.us/download/mjpro150_linux.zip  <span class="o">&amp;&amp;</span> <span class="se">\</span>
        unzip mjpro150_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        wget https://www.roboti.us/download/mujoco200_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        unzip mujoco200_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        mv mujoco200_linux mujoco200

        <span class="c1"># Export global environment variables</span>
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
        cp /mjkey.txt /Gym/.mujoco/mjkey.txt
        <span class="c1"># Install Python dependencies</span>
        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt
        pip install -r requirements.txt
        <span class="c1"># Install Gym and MuJoCo</span>
        <span class="nb">cd</span> /Gym/gym
        pip install -e <span class="s1">&#39;.[all]&#39;</span>
        <span class="c1"># Change permission to use mujoco_py as non sudoer user</span>
        chmod -R <span class="m">777</span> /opt/conda/lib/python3.6/site-packages/mujoco_py/
        pip install --upgrade minerl

<span class="c1"># Export global environment variables</span>
%environment
        <span class="nb">export</span> <span class="nv">SHELL</span><span class="o">=</span>/bin/sh
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
        <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/Gym/gym/.tox/py3/bin:<span class="nv">$PATH</span>

%runscript
        <span class="nb">exec</span> /bin/sh <span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Here is the same recipe but written for TensorFlow:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a dockerfile that sets up a full Gym install with test dependencies</span>
Bootstrap: docker

<span class="c1"># Here we ll build our container upon the tensorflow container</span>
From: tensorflow/tensorflow:latest-gpu-py3

<span class="c1"># Now we&#39;ll copy the mjkey file located in the current directory inside the container&#39;s root</span>
<span class="c1"># directory</span>
%files
        mjkey.txt

<span class="c1"># Then we put everything we need to install</span>
%post
        apt -y update <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt install -y keyboard-configuration <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt install -y <span class="se">\</span>
        python3-setuptools <span class="se">\</span>
        python3-dev <span class="se">\</span>
        python-pyglet <span class="se">\</span>
        python3-opengl <span class="se">\</span>
        libjpeg-dev <span class="se">\</span>
        libboost-all-dev <span class="se">\</span>
        libsdl2-dev <span class="se">\</span>
        libosmesa6-dev <span class="se">\</span>
        patchelf <span class="se">\</span>
        ffmpeg <span class="se">\</span>
        xvfb <span class="se">\</span>
        wget <span class="se">\</span>
        git <span class="se">\</span>
        unzip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        apt clean <span class="o">&amp;&amp;</span> <span class="se">\</span>
        rm -rf /var/lib/apt/lists/*

        <span class="c1"># Download Gym and MuJoCo</span>
        mkdir /Gym <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /Gym
        git clone https://github.com/openai/gym.git <span class="o">||</span> <span class="nb">true</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
        mkdir /Gym/.mujoco <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /Gym/.mujoco
        wget https://www.roboti.us/download/mjpro150_linux.zip  <span class="o">&amp;&amp;</span> <span class="se">\</span>
        unzip mjpro150_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        wget https://www.roboti.us/download/mujoco200_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        unzip mujoco200_linux.zip <span class="o">&amp;&amp;</span> <span class="se">\</span>
        mv mujoco200_linux mujoco200

        <span class="c1"># Export global environment variables</span>
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
        cp /mjkey.txt /Gym/.mujoco/mjkey.txt

        <span class="c1"># Install Python dependencies</span>
        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt
        pip install -r requirements.txt
        <span class="c1"># Install Gym and MuJoCo</span>
        <span class="nb">cd</span> /Gym/gym
        pip install -e <span class="s1">&#39;.[all]&#39;</span>
        <span class="c1"># Change permission to use mujoco_py as non sudoer user</span>
        chmod -R <span class="m">777</span> /usr/local/lib/python3.5/dist-packages/mujoco_py/

        <span class="c1"># Then install miniworld</span>
        <span class="nb">cd</span> /usr/local/
        git clone https://github.com/maximecb/gym-miniworld.git
        <span class="nb">cd</span> gym-miniworld
        pip install -e .

<span class="c1"># Export global environment variables</span>
%environment
        <span class="nb">export</span> <span class="nv">SHELL</span><span class="o">=</span>/bin/bash
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
        <span class="nb">export</span> <span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
        <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/Gym/gym/.tox/py3/bin:<span class="nv">$PATH</span>

%runscript
        <span class="nb">exec</span> /bin/bash <span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Keep in mind that those environment variables are sourced at runtime and not at
build time. This is why, you should also define them in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section
since they are required to install MuJoCo.</p>
</section>
</section>
</section>
<section id="using-containers-on-clusters">
<span id="using-containers"></span><h3>Using containers on clusters<a class="headerlink" href="#using-containers-on-clusters" title="Permalink to this heading"></a></h3>
<section id="how-to-use-containers-on-clusters">
<h4>How to use containers on clusters<a class="headerlink" href="#how-to-use-containers-on-clusters" title="Permalink to this heading"></a></h4>
<p>On every cluster with Slurm, datasets and intermediate results should go in
<code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> while the final experiment results should go in <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>.
In order to use the container you built, you need to copy it on the cluster you
want to use.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should always store your container in $SCRATCH !</p>
</div>
<p>Then reserve a node with srun/sbatch, copy the container and your dataset on the
node given by SLURM (i.e in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>) and execute the code
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code> within the container <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity <span class="nb">exec</span> --nv -H <span class="nv">$HOME</span>:/home -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt; python &lt;YOUR_CODE&gt;</span>
</pre></div></div><p>Remember that <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>, <code class="docutils literal notranslate"><span class="pre">/tmp_log</span></code> and <code class="docutils literal notranslate"><span class="pre">/final_log</span></code> were created in the
previous section. Now each time, we’ll use singularity, we are explicitly
telling it to mount <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> on the cluster’s node in the folder
<code class="docutils literal notranslate"><span class="pre">/dataset</span></code> inside the container with the option <code class="docutils literal notranslate"><span class="pre">-B</span></code> such that each dataset
downloaded by PyTorch in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code> will be available in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>.</p>
<p>This will allow us to have code and scripts that are invariant to the cluster
environment. The option <code class="docutils literal notranslate"><span class="pre">-H</span></code> specify what will be the container’s home. For
example, if you have your code in <code class="docutils literal notranslate"><span class="pre">$HOME/Project12345/Version35/</span></code> you can
specify <code class="docutils literal notranslate"><span class="pre">-H</span> <span class="pre">$HOME/Project12345/Version35:/home</span></code>, thus the container will only
have access to the code inside <code class="docutils literal notranslate"><span class="pre">Version35</span></code>.</p>
<p>If you want to run multiple commands inside the container you can use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity <span class="nb">exec</span> --nv -H <span class="nv">$HOME</span>:/home -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
   -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
   <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt; bash -c <span class="s1">&#39;pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;&#39;</span></span>
</pre></div></div><section id="example-interactive-case-srun-salloc">
<h5>Example: Interactive case (srun/salloc)<a class="headerlink" href="#example-interactive-case-srun-salloc" title="Permalink to this heading"></a></h5>
<p>Once you get an interactive session with SLURM, copy <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> and
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_DATASET&gt;</span></code> to <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt5:before {
  content: "# ";
}
</style><span class="prompt5"><span class="m">0</span>. Get an interactive session</span>
<span class="prompt1">srun --gres<span class="o">=</span>gpu:1</span>
<span class="prompt5"><span class="m">1</span>. Copy your container on the compute node</span>
<span class="prompt1">rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt; <span class="nv">$SLURM_TMPDIR</span></span>
<span class="prompt5"><span class="m">2</span>. Copy your dataset on the compute node</span>
<span class="prompt1">rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt; <span class="nv">$SLURM_TMPDIR</span></span>
</pre></div></div><p>Then use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">shell</span></code> to get a shell inside the container</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">3</span>. Get a shell <span class="k">in</span> your environment</span>
<span class="prompt1">singularity shell --nv <span class="se">\</span>
        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
        <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;</span>
</pre></div></div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">4</span>. Execute your code</span>
<span class="prompt4">python &lt;YOUR_CODE&gt;</span>
</pre></div></div><p><strong>or</strong> use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">exec</span></code> to execute <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">3</span>. Execute your code</span>
<span class="prompt1">singularity <span class="nb">exec</span> --nv <span class="se">\</span>
        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
        <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt; <span class="se">\</span>
        python &lt;YOUR_CODE&gt;</span>
</pre></div></div><p>You can create also the following alias to make your life easier.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">alias</span> <span class="nv">my_env</span><span class="o">=</span><span class="s1">&#39;singularity exec --nv \</span>
<span class="s1">        -H $HOME:/home \</span>
<span class="s1">        -B $SLURM_TMPDIR:/dataset/ \</span>
<span class="s1">        -B $SLURM_TMPDIR:/tmp_log/ \</span>
<span class="s1">        -B $SCRATCH:/final_log/ \</span>
<span class="s1">        $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;&#39;</span></span>
</pre></div></div><p>This will allow you to run any code with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">my_env python &lt;YOUR_CODE&gt;</span>
</pre></div></div></section>
<section id="example-sbatch-case">
<h5>Example: sbatch case<a class="headerlink" href="#example-sbatch-case" title="Permalink to this heading"></a></h5>
<p>You can also create a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>:linenos:

<span class="c1">#!/bin/bash</span>
<span class="c1">#SBATCH --cpus-per-task=6         # Ask for 6 CPUs</span>
<span class="c1">#SBATCH --gres=gpu:1              # Ask for 1 GPU</span>
<span class="c1">#SBATCH --mem=10G                 # Ask for 10 GB of RAM</span>
<span class="c1">#SBATCH --time=0:10:00            # The job will run for 10 minutes</span>

<span class="c1"># 1. Copy your container on the compute node</span>
rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="c1"># 2. Copy your dataset on the compute node</span>
rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="c1"># 3. Executing your code with singularity</span>
singularity <span class="nb">exec</span> --nv <span class="se">\</span>
        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
        <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt; <span class="se">\</span>
        python <span class="s2">&quot;&lt;YOUR_CODE&gt;&quot;</span>
<span class="c1"># 4. Copy whatever you want to save on $SCRATCH</span>
rsync -avz <span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt; <span class="nv">$SCRATCH</span>
</pre></div>
</div>
</section>
<section id="issue-with-pybullet-and-opengl-libraries">
<h5>Issue with PyBullet and OpenGL libraries<a class="headerlink" href="#issue-with-pybullet-and-opengl-libraries" title="Permalink to this heading"></a></h5>
<p>If you are running certain gym environments that require <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you may
encounter a problem when running your singularity instance with the Nvidia
drivers using the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag. This happens because the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag also
provides the OpenGL libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>libGL.so.1 <span class="o">=</span>&gt; /.singularity.d/libs/libGL.so.1
libGLX.so.0 <span class="o">=</span>&gt; /.singularity.d/libs/libGLX.so.0
</pre></div>
</div>
<p>If you don’t experience those problems with <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you probably don’t need
to address this. Otherwise, you can resolve those problems by <code class="docutils literal notranslate"><span class="pre">apt-get</span> <span class="pre">install</span>
<span class="pre">-y</span> <span class="pre">libosmesa6-dev</span> <span class="pre">mesa-utils</span> <span class="pre">mesa-utils-extra</span> <span class="pre">libgl1-mesa-glx</span></code>, and then making
sure that your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> points to those libraries before the ones in
<code class="docutils literal notranslate"><span class="pre">/.singularity.d/libs</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%environment
        <span class="c1"># ...</span>
        <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu/mesa:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</section>
<section id="mila-cluster">
<h5>Mila cluster<a class="headerlink" href="#mila-cluster" title="Permalink to this heading"></a></h5>
<p>On the Mila cluster <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> is not yet defined, you should add the
experiment results you want to keep in <code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;/</span></code>. In
order to use the sbatch script above and to match other cluster environment’s
names, you can define <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> as an alias for
<code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;</span></code> with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">echo</span> <span class="s2">&quot;export SCRATCH=/network/scratch/</span><span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span><span class="s2">/</span><span class="nv">$USER</span><span class="s2">&quot;</span> &gt;&gt; ~/.bashrc</span>
</pre></div></div><p>Then, you can follow the general procedure explained above.</p>
</section>
<section id="compute-canada">
<h5>Compute Canada<a class="headerlink" href="#compute-canada" title="Permalink to this heading"></a></h5>
<p>Using singularity on Compute Canada is similar except that you need to add
Yoshua’s account name and load singularity.  Here is an example of a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>
script using singularity on compute Canada cluster:</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should use singularity/2.6 or singularity/3.4. There is a bug
in singularity/3.2 which makes gpu unusable.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --account=rpp-bengioy     # Yoshua pays for your job</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --cpus-per-task=6         # Ask for 6 CPUs</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --gres=gpu:1              # Ask for 1 GPU</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --mem=32G                 # Ask for 32 GB of RAM</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --time=0:10:00            # The job will run for 10 minutes</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --output=&quot;/scratch/&lt;user&gt;/slurm-%j.out&quot; # Modify the output of sbatch</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># 1. You have to load singularity</span>
<span class="linenos">10</span>module load singularity
<span class="linenos">11</span><span class="c1"># 2. Then you copy the container to the local disk</span>
<span class="linenos">12</span>rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">13</span><span class="c1"># 3. Copy your dataset on the compute node</span>
<span class="linenos">14</span>rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">15</span><span class="c1"># 4. Executing your code with singularity</span>
<span class="linenos">16</span>singularity <span class="nb">exec</span> --nv <span class="se">\</span>
<span class="linenos">17</span>        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
<span class="linenos">18</span>        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
<span class="linenos">19</span>        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
<span class="linenos">20</span>        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
<span class="linenos">21</span>        <span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt; <span class="se">\</span>
<span class="linenos">22</span>        python <span class="s2">&quot;&lt;YOUR_CODE&gt;&quot;</span>
<span class="linenos">23</span><span class="c1"># 5. Copy whatever you want to save on $SCRATCH</span>
<span class="linenos">24</span>rsync -avz <span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt; <span class="nv">$SCRATCH</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="sharing-data-with-acls">
<h2>Sharing Data with ACLs<a class="headerlink" href="#sharing-data-with-acls" title="Permalink to this heading"></a></h2>
<p>Regular permissions bits are extremely blunt tools: They control access through
only three sets of bits owning user, owning group and all others. Therefore,
access is either too narrow (<code class="docutils literal notranslate"><span class="pre">0700</span></code> allows access only by oneself) or too wide
(<code class="docutils literal notranslate"><span class="pre">770</span></code> gives all permissions to everyone in the same group, and <code class="docutils literal notranslate"><span class="pre">777</span></code> to
literally everyone).</p>
<p>ACLs (Access Control Lists) are an expansion of the permissions bits that allow
more fine-grained, granular control of accesses to a file. They can be used to
permit specific users access to files and folders even if conservative default
permissions would have denied them such access.</p>
<p>As an illustrative example, to use ACLs to allow <code class="docutils literal notranslate"><span class="pre">$USER</span></code> (<strong>oneself</strong>) to
share with <code class="docutils literal notranslate"><span class="pre">$USER2</span></code> (<strong>another person</strong>) a “playground” folder hierarchy in
Mila’s scratch filesystem at a location</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">/network/scratch/${USER:0:1}/$USER/X/Y/Z/...</span></code></p>
</div></blockquote>
<p>in a safe and secure fashion that allows both users to read, write, execute,
search and delete each others’ files:</p>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>1.</strong> Grant <strong>oneself</strong> permissions to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl -Rdm user:<span class="nv">$USER</span>:rwx  /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/X/Y/Z/
</pre></div>
</div>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The importance of doing this seemingly-redundant step first is that files
and folders are <strong>always</strong> owned by only one person, almost always their
creator (the UID will be the creator’s, the GID typically as well). If that
user is not yourself, you will not have access to those files unless the
other person specifically gives them to you – or these files inherited a
default ACL allowing you full access.</p>
<p><strong>This</strong> is the inherited, default ACL serving that purpose.</p>
</div>
<div class="line-block">
<div class="line"><strong>2.</strong> Grant <strong>the other</strong> permission to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl -Rdm user:<span class="nv">$USER2</span>:rwx /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/X/Y/Z/
</pre></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>3.</strong> Grant <strong>the other</strong> permission to access any <strong>existing</strong> files/folders created
by <em>oneself</em>.</div>
<div class="line">Such files and folders were created before the new default ACLs were added
above and thus did not inherit them from their parent folder at the moment of
their creation.</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl -Rm  user:<span class="nv">$USER2</span>:rwx /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/X/Y/Z/
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The purpose of granting permissions first for <em>future</em> files and then for
<em>existing</em> files is to prevent a <strong>race condition</strong> whereby after the first
<code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command the other person could create files to which the
second <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command does not apply.</p>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>4.</strong> Grant <strong>another</strong> permission to search through one’s hierarchy down to the
shared location in question.</div>
</div>
<ul class="simple">
<li><p><strong>Non</strong>-recursive (!!!!)</p></li>
<li><p>May also grant <code class="docutils literal notranslate"><span class="pre">:rx</span></code> in unlikely event others listing your folders on the
path is not troublesome or desirable.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl -m   user:<span class="nv">$USER2</span>:x   /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/X/Y/
setfacl -m   user:<span class="nv">$USER2</span>:x   /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/X/
setfacl -m   user:<span class="nv">$USER2</span>:x   /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to access a file, all folders from the root (<code class="docutils literal notranslate"><span class="pre">/</span></code>) down to the
parent folder in question must be searchable (<code class="docutils literal notranslate"><span class="pre">+x</span></code>) by the concerned user.
This is already the case for all users for folders such as <code class="docutils literal notranslate"><span class="pre">/</span></code>,
<code class="docutils literal notranslate"><span class="pre">/network</span></code> and <code class="docutils literal notranslate"><span class="pre">/network/scratch</span></code>, but users must explicitly grant access
to some or all users either through base permissions or by adding ACLs, for
at least <code class="docutils literal notranslate"><span class="pre">/network/scratch/${USER:0:1}/$USER</span></code>, <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> and subfolders.</p>
<p>To bluntly allow <strong>all</strong> users to search through a folder (<strong>think twice!</strong>),
the following command can be used:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chmod a+x /network/scratch/<span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span>/<span class="nv">$USER</span>/
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> and path resolution/access checking,
consider the following documentation viewing commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">setfacl</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">path_resolution</span></code></p></li>
</ul>
</div>
<section id="viewing-and-verifying-acls">
<h3>Viewing and Verifying ACLs<a class="headerlink" href="#viewing-and-verifying-acls" title="Permalink to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>getfacl /path/to/folder/or/file
           <span class="m">1</span>:  <span class="c1"># file: somedir/</span>
           <span class="m">2</span>:  <span class="c1"># owner: lisa</span>
           <span class="m">3</span>:  <span class="c1"># group: staff</span>
           <span class="m">4</span>:  <span class="c1"># flags: -s-</span>
           <span class="m">5</span>:  user::rwx
           <span class="m">6</span>:  user:joe:rwx               <span class="c1">#effective:r-x</span>
           <span class="m">7</span>:  group::rwx                 <span class="c1">#effective:r-x</span>
           <span class="m">8</span>:  group:cool:r-x
           <span class="m">9</span>:  mask::r-x
          <span class="m">10</span>:  other::r-x
          <span class="m">11</span>:  default:user::rwx
          <span class="m">12</span>:  default:user:joe:rwx       <span class="c1">#effective:r-x</span>
          <span class="m">13</span>:  default:group::r-x
          <span class="m">14</span>:  default:mask::r-x
          <span class="m">15</span>:  default:other::---
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">getfacl</span></code></p></li>
</ul>
</div>
</section>
</section>
<section id="contributing-datasets">
<h2>Contributing datasets<a class="headerlink" href="#contributing-datasets" title="Permalink to this heading"></a></h2>
<p>If a dataset could help the research of others at Mila, <a class="reference external" href="https://forms.gle/vDVwD2rZBmYHENgZA">this form</a> can be filled to request its addition
to <a class="reference external" href="Information.html#storage">/network/datasets</a>.</p>
<p>Those datasets can be mirrored to the Béluga cluster in
<code class="docutils literal notranslate"><span class="pre">~/projects/rrg-bengioy-ad/data/curated/</span></code> if they follow Compute Canada’s
<a class="reference external" href="https://docs.computecanada.ca/wiki/AI_and_Machine_Learning#Managing_your_datasets">good practices on data</a>.</p>
<section id="publicly-share-a-mila-dataset">
<h3>Publicly share a Mila dataset<a class="headerlink" href="#publicly-share-a-mila-dataset" title="Permalink to this heading"></a></h3>
<p>Mila offers two ways to publicly share a Mila dataset:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://academictorrents.com">Academic Torrent</a></p></li>
<li><p><a class="reference external" href="https://drive.google.com/drive/folders/1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt">Google Drive</a></p></li>
</ul>
<p>Note that these options are not mutually exclusive and both can be used.</p>
<section id="id5">
<h4>Academic Torrent<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h4>
<p>Mila hosts/seeds some datasets created by the Mila community through <a class="reference external" href="https://academictorrents.com">Academic
Torrent</a>. The first step is to create <a class="reference external" href="https://academictorrents.com/upload.php">an
account and a torrent file</a>.</p>
<p>Then drop the dataset in <code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code> and send the
Academic Torrent URL to <a class="reference external" href="https://it-support.mila.quebec">Mila’s helpdesk</a>. If
the dataset does not reside on the Mila cluster, only the Academic Torrent URL
would be needed to proceed with the initial download. Then you can delete /
stop sharing your copy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Avoid mentioning <em>dataset</em> in the name of the dataset</p></li>
<li><p>Avoid capital letters, special charaters (including spaces) in files and
directories names. Spaces can be replaced by hyphens (<code class="docutils literal notranslate"><span class="pre">-</span></code>).</p></li>
<li><p>Multiple archives can be provided to spread the data (e.g. dataset splits,
raw data, extra data, …)</p></li>
</ul>
</div>
<section id="generate-a-torrent-file-to-be-uploaded-to-academic-torrent">
<h5>Generate a .torrent file to be uploaded to Academic Torrent<a class="headerlink" href="#generate-a-torrent-file-to-be-uploaded-to-academic-torrent" title="Permalink to this heading"></a></h5>
<p>The command line / Python utility <a class="reference external" href="https://github.com/idlesign/torrentool">torrentool</a> can be used to create a
<cite>DATASET_NAME.torrent</cite> file:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install torrentool</span>
python3 -m pip install torrentool click
<span class="c1"># Change Directory to the location of the dataset to be hosted by Mila</span>
<span class="nb">cd</span> /network/scratch/.transit_datasets
torrent create --tracker https://academictorrents.com/announce.php DATASET_NAME
</pre></div>
</div>
<p>The resulting <cite>DATASET_NAME.torrent</cite> can then be used to register a new dataset
on Academic Torrent.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p>The creation of a <cite>DATASET_NAME.torrent</cite> file requires the computation of
checksums for the dataset content which can quickly become CPU-heavy. This
process should <em>not</em> be executed on a login node</p></li>
</ul>
</div>
</section>
<section id="download-a-dataset-from-academic-torrent">
<h5>Download a dataset from Academic Torrent<a class="headerlink" href="#download-a-dataset-from-academic-torrent" title="Permalink to this heading"></a></h5>
<p>Academic Torrent provides a <a class="reference external" href="https://github.com/academictorrents/at-python">Python API</a> to easily download a dataset
from it’s registered list:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the Python API with:</span>
<span class="c1"># python3 -m pip install academictorrents</span>
<span class="kn">import</span> <span class="nn">academictorrents</span> <span class="k">as</span> <span class="nn">at</span>
<span class="n">mnist_path</span> <span class="o">=</span> <span class="n">at</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;323a0048d87ca79b68f12a6350a57776b6a3b7fb&quot;</span><span class="p">,</span> <span class="n">datastore</span><span class="o">=</span><span class="s2">&quot;~/scratch/.academictorrents-datastore&quot;</span><span class="p">)</span> <span class="c1"># Download the mnist dataset</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Current needs have been evaluated to be for a download speed of about 10
MB/s. This speed can be higher if more users also seeds the dataset.</p>
</div>
</section>
</section>
<section id="id7">
<h4>Google Drive<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h4>
<p>Only a member of the staff team can upload to <a class="reference external" href="https://drive.google.com/drive/folders/1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt">Mila’s Google Drive</a>
which requires to first drop the dataset in
<code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code>. Then, contact <a class="reference external" href="https://it-support.mila.quebec">Mila’s helpdesk</a> and provide the following informations:</p>
<ul class="simple">
<li><p>directory containing the archived dataset (zip is favored) in
<code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code></p></li>
<li><p>the name of the dataset</p></li>
<li><p>a licence in <code class="docutils literal notranslate"><span class="pre">.txt</span></code> format. One of the <a class="reference external" href="https://creativecommons.org/about/cclicenses/">the creative common</a> licenses can be used. It is
recommended to at least have the <em>Attribution</em> option. The <em>No Derivatives</em>
option is discouraged unless the dataset should not be modified by others.</p></li>
<li><p>MD5 checksum of the archive</p></li>
<li><p>the arXiv and GitHub URLs (those can be sent later if the article is still in
the submission process)</p></li>
<li><p>instructions to know if the dataset needs to be <code class="docutils literal notranslate"><span class="pre">unzip</span></code>ed, <code class="docutils literal notranslate"><span class="pre">untar</span></code>ed or
else before uploading to Google Drive</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Avoid mentioning <em>dataset</em> in the name of the dataset</p></li>
<li><p>Avoid capital letters, special charaters (including spaces) in files and
directories names. Spaces can be replaced by hyphens (<code class="docutils literal notranslate"><span class="pre">-</span></code>).</p></li>
<li><p>Multiple archives can be provided to spread the data (e.g. dataset splits,
raw data, extra data, …)</p></li>
</ul>
</div>
<section id="download-a-dataset-from-mila-s-google-drive-with-gdown">
<h5>Download a dataset from Mila’s Google Drive with  <code class="docutils literal notranslate"><span class="pre">gdown</span></code><a class="headerlink" href="#download-a-dataset-from-mila-s-google-drive-with-gdown" title="Permalink to this heading"></a></h5>
<p>The utility <a class="reference external" href="https://github.com/wkentaro/gdown">gdown</a> is a simple utility to
download data from Google Drive from the command line shell or in a Python
script and requires no setup.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A limitation however is that it uses a shared client id which can cause a
quota block when too many users uses it in the same day. It is described in
a <a class="reference external" href="https://github.com/wkentaro/gdown/issues/43#issuecomment-642182100">GitHub issue</a>.</p>
</div>
</section>
<section id="download-a-dataset-from-mila-s-google-drive-with-rclone">
<h5>Download a dataset from Mila’s Google Drive with <code class="docutils literal notranslate"><span class="pre">rclone</span></code><a class="headerlink" href="#download-a-dataset-from-mila-s-google-drive-with-rclone" title="Permalink to this heading"></a></h5>
<p><a class="reference external" href="https://rclone.org/">Rclone</a> is a command line program to manage files on
cloud storage. In the context of a Google Drive remote, it allows to specify a
client id to avoid sharing with other users which avoid quota limits. Rclone
describes the creation of a <a class="reference external" href="https://rclone.org/drive/#making-your-own-client-id">client id in its documentaton</a>. Once this is done, a
remote for Mila’s Google Drive can be configured from the command line:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>rclone config create mila-gdrive drive client_id XXXXXXXXXXXX-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.apps.googleusercontent.com <span class="se">\</span>
    client_secret XXXXXXXXXXXXX-XXXXXXXXXX <span class="se">\</span>
    scope <span class="s1">&#39;drive.readonly&#39;</span> <span class="se">\</span>
    root_folder_id 1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt <span class="se">\</span>
    config_is_local <span class="nb">false</span> <span class="se">\</span>
    config_refresh_token <span class="nb">false</span>
</pre></div>
</div>
<p>The remote can then be used to download a dataset:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>rclone copy --progress mila-gdrive:DATASET_NAME/ ~/scratch/datasets/DATASET_NAME/
</pre></div>
</div>
<p>Rclone is available from the <a class="reference external" href="https://anaconda.org/conda-forge/rclone">conda channel conda-forge</a>.</p>
</section>
</section>
<section id="digital-object-identifier-doi">
<h4>Digital Object Identifier (DOI)<a class="headerlink" href="#digital-object-identifier-doi" title="Permalink to this heading"></a></h4>
<p>It is recommended to get a DOI to reference the dataset. A DOI is a permanent
id/URL which prevents losing references of online scientific data.
<a class="reference external" href="https://figshare.com">https://figshare.com</a> can be used to create a DOI:</p>
<ul class="simple">
<li><p>Go in <cite>My Data</cite></p></li>
<li><p>Create an item by clicking <cite>Create new item</cite></p></li>
<li><p>Check <cite>Metadata record only</cite> at the top</p></li>
<li><p>Fill the metadata fields</p></li>
</ul>
<p>Then reference the dataset using <a class="reference external" href="https://doi.org">https://doi.org</a> like this:
<a class="reference external" href="https://doi.org/10.6084/m9.figshare.2066037">https://doi.org/10.6084/m9.figshare.2066037</a></p>
</section>
</section>
</section>
<section id="data-transmission-using-globus-connect-personal">
<h2>Data Transmission using Globus Connect Personal<a class="headerlink" href="#data-transmission-using-globus-connect-personal" title="Permalink to this heading"></a></h2>
<p>Mila doesn’t own a Globus license but if the source or destination provides a
Globus account, like Compute Canada for example, it’s possible to setup Globus
Connect Personal to create a personal endpoint on the Mila cluster by following
the Globus guide to <a class="reference external" href="https://docs.globus.org/how-to/globus-connect-personal-linux/">Install, Configure, and Uninstall Globus Connect Personal
for Linux</a>.</p>
<p>This endpoint can then be used to transfer data to and from the Mila cluster.</p>
</section>
<section id="jupyterhub">
<h2>JupyterHub<a class="headerlink" href="#jupyterhub" title="Permalink to this heading"></a></h2>
<p><strong>JupyterHub</strong> is a platform connected to SLURM to start a <strong>JupyterLab</strong>
session as a batch job then connects it when the allocation has been granted.
It does not require any ssh tunnel or port redirection, the hub acts as a proxy
server that will redirect you to a session as soon as it is available.</p>
<p>It is currently available for Mila clusters and some Compute Canada clusters</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 63%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Cluster</p></th>
<th class="head"><p>Address</p></th>
<th class="head"><p>Login type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mila Local</p></td>
<td><p><a class="reference external" href="https://jupyterhub.server.mila.quebec">https://jupyterhub.server.mila.quebec</a></p></td>
<td><p>Google Oauth</p></td>
</tr>
<tr class="row-odd"><td><p>Compute Canada</p></td>
<td><p><a class="reference external" href="https://docs.computecanada.ca/wiki/JupyterHub">https://docs.computecanada.ca/wiki/JupyterHub</a></p></td>
<td><p>CC login</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not forget to close the JupyterLab session! Closing the window leaves
running the session and the SLURM job it is linked to.</p>
<p>To close it, use the <code class="docutils literal notranslate"><span class="pre">hub</span></code> menu and then <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Panel</span> <span class="pre">&gt;</span> <span class="pre">Stop</span> <span class="pre">my</span> <span class="pre">server</span></code></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>For Mila Clusters:</strong></p>
<p><em>mila.quebec</em> account credentials should be used to login and start a
<strong>JupyterLab</strong> session.</p>
</div>
<section id="access-mila-storage-in-jupyterlab">
<h3>Access Mila Storage in JupyterLab<a class="headerlink" href="#access-mila-storage-in-jupyterlab" title="Permalink to this heading"></a></h3>
<p>Unfortunately, JupyterLab does not allow the navigation to parent directories of
<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>. This makes some file systems like <code class="docutils literal notranslate"><span class="pre">/network/datasets</span></code> or
<code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> unavailable through their absolute path in the interface. It
is however possible to create symbolic links to those resources. To do so, you
can use the <code class="docutils literal notranslate"><span class="pre">ln</span> <span class="pre">-s</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ln -s /network/datasets <span class="nv">$HOME</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> is a directory that is dynamically created for each
job so you would need to recreate the symbolic link every time you start a
JupyterHub session:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ln -sf <span class="nv">$SLURM_TMPDIR</span> <span class="nv">$HOME</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-slurm-usage-and-multiple-gpu-jobs">
<h2>Advanced SLURM usage and Multiple GPU jobs<a class="headerlink" href="#advanced-slurm-usage-and-multiple-gpu-jobs" title="Permalink to this heading"></a></h2>
<section id="handling-preemption">
<h3>Handling preemption<a class="headerlink" href="#handling-preemption" title="Permalink to this heading"></a></h3>
<p id="advanced-preemption">On the Mila cluster, jobs can preempt one-another depending on their priority
(unkillable&gt;high&gt;low) (See the <a class="reference external" href="https://slurm.schedmd.com/preempt.html">Slurm documentation</a>)</p>
<p>The default preemption mechanism is to kill and re-queue the job automatically
without any notice. To allow a different preemption mechanism, every partition
have been duplicated (i.e. have the same characteristics as their counterparts)
allowing a <strong>120sec</strong> grace period before killing your job <em>but don’t requeue
it automatically</em>: those partitions are referred by the suffix: <code class="docutils literal notranslate"><span class="pre">-grace</span></code>
(<code class="docutils literal notranslate"><span class="pre">main-grace,</span> <span class="pre">low-grace,</span> <span class="pre">cpu_jobs-grace</span></code>).</p>
<p>When using a partition with a grace period, a series of signals consisting of
first <code class="docutils literal notranslate"><span class="pre">SIGCONT</span></code> and <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> then <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code> will be sent to the SLURM
job.  It’s good practice to catch those signals using the Linux <code class="docutils literal notranslate"><span class="pre">trap</span></code> command
to properly terminate a job and save what’s necessary to restart the job.  On
each cluster, you’ll be allowed a <em>grace period</em> before SLURM actually kills
your job (<code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>).</p>
<p>The easiest way to handle preemption is by trapping the <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> signal</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">#SBATCH --ntasks=1</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH ....</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span>exit_script<span class="o">()</span> <span class="o">{</span>
<span class="linenos"> 5</span>    <span class="nb">echo</span> <span class="s2">&quot;Preemption signal, saving myself&quot;</span>
<span class="linenos"> 6</span>    <span class="nb">trap</span> - SIGTERM <span class="c1"># clear the trap</span>
<span class="linenos"> 7</span>    <span class="c1"># Optional: sends SIGTERM to child/sub processes</span>
<span class="linenos"> 8</span>    <span class="nb">kill</span> -- -<span class="nv">$$</span>
<span class="linenos"> 9</span><span class="o">}</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="nb">trap</span> exit_script SIGTERM
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># The main script part</span>
<span class="linenos">14</span>python3 my_script
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line"><strong>Requeuing</strong>:</div>
<div class="line">The Slurm scheduler on the cluster does not allow a grace period before</div>
<div class="line">preempting a job while requeuing it automatically, therefore your job will</div>
<div class="line">be cancelled at the end of the grace period.</div>
<div class="line">To automatically requeue it, you can just add the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command inside</div>
<div class="line">your <code class="docutils literal notranslate"><span class="pre">exit_script</span></code> function.</div>
</div>
</div>
</section>
<section id="packing-jobs">
<h3>Packing jobs<a class="headerlink" href="#packing-jobs" title="Permalink to this heading"></a></h3>
<section id="sharing-a-gpu-between-processes">
<h4>Sharing a GPU between processes<a class="headerlink" href="#sharing-a-gpu-between-processes" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">srun</span></code>, when used in a batch job is responsible for starting tasks on the
allocated resources (see srun) SLURM batch script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">#SBATCH --ntasks-per-node=2</span>
<span class="linenos">2</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos">3</span><span class="c1">#SBATCH --ntasks=2</span>
<span class="linenos">4</span><span class="c1">#SBATCH --gres=gpu:1</span>
<span class="linenos">5</span><span class="c1">#SBATCH --cpus-per-task=4</span>
<span class="linenos">6</span><span class="c1">#SBATCH --mem=18G</span>
<span class="linenos">7</span>srun -l --output<span class="o">=</span>myjob_output_%t.out python script args
</pre></div>
</div>
<p>This will run Python 2 times, each process with 4 CPUs with the same arguments
<code class="docutils literal notranslate"><span class="pre">--output=myjob_output_%t.out</span></code> will create 2 output files appending the task
id (<code class="docutils literal notranslate"><span class="pre">%t</span></code>) to the filename and 1 global log file for things happening outside
the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p>
<p>Knowing that, if you want to have 2 different arguments to the Python program,
you can use a multi-prog configuration file: <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-l</span> <span class="pre">--multi-prog</span> <span class="pre">silly.conf</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>  <span class="n">python</span> <span class="n">script</span> <span class="n">firstarg</span>
<span class="mi">1</span>  <span class="n">python</span> <span class="n">script</span> <span class="n">secondarg</span>
</pre></div>
</div>
<p>Or by specifying a range of tasks</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span><span class="o">-</span><span class="mi">1</span>  <span class="n">python</span> <span class="n">script</span> <span class="o">%</span><span class="n">t</span>
</pre></div>
</div>
<p>%t being the taskid that your Python script will parse.  Note the <code class="docutils literal notranslate"><span class="pre">-l</span></code> on the
<code class="docutils literal notranslate"><span class="pre">srun</span></code> command: this will prepend each line with the taskid (0:, 1:)</p>
</section>
<section id="sharing-a-node-with-multiple-gpu-1process-gpu">
<h4>Sharing a node with multiple GPU 1process/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-1process-gpu" title="Permalink to this heading"></a></h4>
<p>On Compute Canada, several nodes, especially nodes with <code class="docutils literal notranslate"><span class="pre">largeGPU</span></code> (P100) are
reserved for jobs requesting the whole node, therefore packing multiple
processes in a single job can leverage faster GPU.</p>
<p>If you want different tasks to access different GPUs in a single allocation you
need to create an allocation requesting a whole node and using <code class="docutils literal notranslate"><span class="pre">srun</span></code> with a
subset of those resources (1 GPU).</p>
<p>Keep in mind that every resource not specified on the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command while
inherit the global allocation specification so you need to split each resource
in a subset (except –cpu-per-task which is a per-task requirement)</p>
<p>Each <code class="docutils literal notranslate"><span class="pre">srun</span></code> represents a job step (<code class="docutils literal notranslate"><span class="pre">%s</span></code>).</p>
<p>Example for a GPU node with 24 cores and 4 GPUs and 128G of RAM
Requesting 1 task per GPU</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --nodes=1-1</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:4</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --cpus-per-task=6</span>
<span class="linenos"> 7</span>srun --gres<span class="o">=</span>gpu:1 -n1 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s.out --exclusive --multi-prog python script args1 <span class="p">&amp;</span>
<span class="linenos"> 8</span>srun --gres<span class="o">=</span>gpu:1 -n1 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s.out --exclusive --multi-prog python script args2 <span class="p">&amp;</span>
<span class="linenos"> 9</span>srun --gres<span class="o">=</span>gpu:1 -n1 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s.out --exclusive --multi-prog python script args3 <span class="p">&amp;</span>
<span class="linenos">10</span>srun --gres<span class="o">=</span>gpu:1 -n1 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s.out --exclusive --multi-prog python script args4 <span class="p">&amp;</span>
<span class="linenos">11</span><span class="nb">wait</span>
</pre></div>
</div>
<p>This will create 4 output files:</p>
<ul class="simple">
<li><p>JOBID-step-0.out</p></li>
<li><p>JOBID-step-1.out</p></li>
<li><p>JOBID-step-2.out</p></li>
<li><p>JOBID-step-3.out</p></li>
</ul>
</section>
<section id="sharing-a-node-with-multiple-gpu-multiple-processes-gpu">
<h4>Sharing a node with multiple GPU &amp; multiple processes/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu" title="Permalink to this heading"></a></h4>
<p>Combining both previous sections, we can create a script requesting a whole node
with four GPUs, allocating 1 GPU per <code class="docutils literal notranslate"><span class="pre">srun</span></code> and sharing each GPU with multiple
processes</p>
<p>Example still with a 24 cores/4 GPUs/128G RAM
Requesting 2 tasks per GPU</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --nodes=1-1</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --ntasks-per-node=8</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:4</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --cpus-per-task=3</span>
<span class="linenos"> 7</span>srun --gres<span class="o">=</span>gpu:1 -n2 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf <span class="p">&amp;</span>
<span class="linenos"> 8</span>srun --gres<span class="o">=</span>gpu:1 -n2 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf <span class="p">&amp;</span>
<span class="linenos"> 9</span>srun --gres<span class="o">=</span>gpu:1 -n2 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf <span class="p">&amp;</span>
<span class="linenos">10</span>srun --gres<span class="o">=</span>gpu:1 -n2 --mem<span class="o">=</span>30G -l --output<span class="o">=</span>%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf <span class="p">&amp;</span>
<span class="linenos">11</span><span class="nb">wait</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> is important to specify subsequent step/srun to bind to different cpus.</p>
<p>This will produce 8 output files, 2 for each step:</p>
<ul class="simple">
<li><p>JOBID-step-0-task-0.out</p></li>
<li><p>JOBID-step-0-task-1.out</p></li>
<li><p>JOBID-step-1-task-0.out</p></li>
<li><p>JOBID-step-1-task-1.out</p></li>
<li><p>JOBID-step-2-task-0.out</p></li>
<li><p>JOBID-step-2-task-1.out</p></li>
<li><p>JOBID-step-3-task-0.out</p></li>
<li><p>JOBID-step-3-task-1.out</p></li>
</ul>
<p>Running <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> in silly.conf, while parsing the output, we can see 4
GPUs allocated and 2 tasks per GPU</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">cat JOBID-step-* <span class="p">|</span> grep Tesla
<span class="m">0</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:04:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">1</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:04:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">0</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:83:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">1</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:83:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">0</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:82:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">1</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:82:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">0</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:03:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span>
<span class="m">1</span>: <span class="p">|</span>   <span class="m">0</span>  Tesla P100-PCIE...  On   <span class="p">|</span> <span class="m">00000000</span>:03:00.0 Off <span class="p">|</span>                    <span class="m">0</span> <span class="p">|</span></span>
</pre></div></div></section>
</section>
</section>
<section id="frequently-asked-questions-faqs">
<h2>Frequently asked questions (FAQs)<a class="headerlink" href="#frequently-asked-questions-faqs" title="Permalink to this heading"></a></h2>
<section id="connection-ssh-issues">
<h3>Connection/SSH issues<a class="headerlink" href="#connection-ssh-issues" title="Permalink to this heading"></a></h3>
<section id="i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node<a class="headerlink" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node" title="Permalink to this heading"></a></h4>
<p>Login nodes are protected against brute force attacks and might ban your IP if
it detects too many connections/failures. You will be automatically unbanned
after 1 hour. For any further problem, please <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket.</a></p>
</section>
</section>
<section id="shell-issues">
<h3>Shell issues<a class="headerlink" href="#shell-issues" title="Permalink to this heading"></a></h3>
<section id="how-do-i-change-my-shell">
<h4>How do I change my shell ?<a class="headerlink" href="#how-do-i-change-my-shell" title="Permalink to this heading"></a></h4>
<p>By default you will be assigned <code class="docutils literal notranslate"><span class="pre">/bin/bash</span></code> as a shell. If you would like to
change for another one, please <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket.</a></p>
</section>
</section>
<section id="slurm-issues">
<h3>SLURM issues<a class="headerlink" href="#slurm-issues" title="Permalink to this heading"></a></h3>
<section id="how-can-i-get-an-interactive-shell-on-the-cluster">
<h4>How can I get an interactive shell on the cluster ?<a class="headerlink" href="#how-can-i-get-an-interactive-shell-on-the-cluster" title="Permalink to this heading"></a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">[--slurm_options]</span></code> without any executable at the end of the
command, this will launch your default shell on an interactive session. Remember
that an interactive session is bound to the login node where you start it so you
could risk losing your job if the login node becomes unreachable.</p>
</section>
<section id="how-can-i-reset-my-cluster-password">
<h4>How can I reset my cluster password ?<a class="headerlink" href="#how-can-i-reset-my-cluster-password" title="Permalink to this heading"></a></h4>
<p>To reset your password, please <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket.</a></p>
<p><strong>Warning</strong>: your cluster password is the same as your Google Workspace account. So,
after reset, you must use the new password for all your Google services.</p>
</section>
<section id="srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">
<h4>srun: error: –mem and –mem-per-cpu are mutually exclusive<a class="headerlink" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive" title="Permalink to this heading"></a></h4>
<p>You can safely ignore this, <code class="docutils literal notranslate"><span class="pre">salloc</span></code> has a default memory flag in case you
don’t provide one.</p>
</section>
<section id="how-can-i-see-where-and-if-my-jobs-are-running">
<h4>How can I see where and if my jobs are running ?<a class="headerlink" href="#how-can-i-see-where-and-if-my-jobs-are-running" title="Permalink to this heading"></a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code> to see all your job status and locations.
To get more info on a running job, try <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">#JOBID</span></code></p>
</section>
<section id="unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">
<h4>Unable to allocate resources: Invalid account or account/partition combination specified<a class="headerlink" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified" title="Permalink to this heading"></a></h4>
<p>Chances are your account is not setup properly. You should <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket.</a></p>
</section>
<section id="how-do-i-cancel-a-job">
<h4>How do I cancel a job?<a class="headerlink" href="#how-do-i-cancel-a-job" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>To cancel a specific job, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">#JOBID</span></code></p></li>
<li><p>To cancel all your jobs (running and pending), use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code></p></li>
<li><p>To cancel all your pending jobs only, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-t</span> <span class="pre">PD</span></code></p></li>
</ul>
</section>
<section id="how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">
<h4>How can I access a node on which one of my jobs is running ?<a class="headerlink" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running" title="Permalink to this heading"></a></h4>
<p>You can ssh into a node on which you have a job running, your ssh connection
will be adopted by your job, i.e.  if your job finishes your ssh connection will
be automatically terminated. In order to connect to a node, you need to have
password-less ssh either with a key present in your home or with an
<code class="docutils literal notranslate"><span class="pre">ssh-agent</span></code>. You can generate a key on the login node like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ssh-keygen <span class="o">(</span>3xENTER<span class="o">)</span></span>
<span class="prompt1">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span>
<span class="prompt1">chmod <span class="m">600</span> ~/.ssh/authorized_keys</span>
<span class="prompt1">chmod <span class="m">700</span> ~/.ssh</span>
</pre></div></div></section>
<section id="i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node<a class="headerlink" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node" title="Permalink to this heading"></a></h4>
<p>See previous question</p>
</section>
<section id="where-do-i-put-my-data-during-a-job">
<h4>Where do I put my data during a job ?<a class="headerlink" href="#where-do-i-put-my-data-during-a-job" title="Permalink to this heading"></a></h4>
<p>Your <code class="docutils literal notranslate"><span class="pre">/home</span></code> as well as the datasets are on shared file-systems, it is
recommended to copy them to the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> to better process them and
leverage higher-speed local drives. If you run a low priority job subject to
preemption, it’s better to save any output you want to keep on the shared file
systems, because the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> is deleted at the end of each job.</p>
</section>
<section id="slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">
<h4>slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup<a class="headerlink" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup" title="Permalink to this heading"></a></h4>
<p>You exceeded the amount of memory allocated to your job, either you did not
request enough memory or you have a memory leak in your process. Try increasing
the amount of memory requested with <code class="docutils literal notranslate"><span class="pre">--mem=</span></code> or <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu=</span></code>.</p>
</section>
<section id="fork-retry-resource-temporarily-unavailable">
<h4>fork: retry: Resource temporarily unavailable<a class="headerlink" href="#fork-retry-resource-temporarily-unavailable" title="Permalink to this heading"></a></h4>
<p>You exceeded the limit of 2000 tasks/PIDs in your job, it probably means there
is an issue with a sub-process spawning too many processes in your script. For
any help with your software, please <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket.</a></p>
</section>
</section>
<section id="pytorch-issues">
<h3>PyTorch issues<a class="headerlink" href="#pytorch-issues" title="Permalink to this heading"></a></h3>
<section id="i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">
<h4>I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code><a class="headerlink" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263" title="Permalink to this heading"></a></h4>
<p>You are using PyTorch 1.10.x and hitting <a class="reference external" href="https://github.com/pytorch/pytorch/issues/67864">#67864</a>,
for which the solution is <a class="reference external" href="https://github.com/pytorch/pytorch/pull/72232">PR #72232</a>
merged in PyTorch 1.11.x. For an immediate fix, consider the following compilable Gist:
<a class="reference external" href="https://gist.github.com/obilaniu/b133470cb70410d841faca819d3921e5">hack.cpp</a>.
Compile the patch to <code class="docutils literal notranslate"><span class="pre">hack.so</span></code> and then <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_PRELOAD=/absolute/path/to/hack.so</span></code>
before executing the Python process that <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch</span></code> a broken PyTorch 1.10.</p>
<p>For Hydra users who are using the submitit launcher plug-in, the <code class="docutils literal notranslate"><span class="pre">env_set</span></code> key cannot
be used to set <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> in the environment as it does so too late at runtime. The
dynamic loader reads <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> only once and very early during the startup of any
process, before the variable can be set from inside the process. The hack must therefore
be injected using the <code class="docutils literal notranslate"><span class="pre">setup</span></code> key in Hydra YAML config file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hydra</span><span class="p">:</span>
  <span class="n">launcher</span><span class="p">:</span>
    <span class="n">setup</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">export</span> <span class="n">LD_PRELOAD</span><span class="o">=/</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">hack</span><span class="o">.</span><span class="n">so</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Extra_compute.html" class="btn btn-neutral float-left" title="Computational resources outside of Mila" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Handbook.html" class="btn btn-neutral float-right" title="AI tooling and methodology handbook" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<script type="text/javascript">
  window.onload = function() {
      $(".toggle > *").hide();
      $(".toggle .header").show();
      $(".toggle .header").click(function() {
          $(this).parent().children().not(".header").toggle(400);
          $(this).parent().children(".header").toggleClass("open");
      })
  };
</script>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>