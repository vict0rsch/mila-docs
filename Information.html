<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Computing infrastructure and policies &mdash; MILA Technical Documentation latest documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/documentation_options.js"></script>
        <script src="_static/documentation_options_fix.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Computational resources outside of Mila" href="Extra_compute.html" />
    <link rel="prev" title="What is a computer cluster?" href="Theory_cluster.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/image.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                latest
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html">Purpose of this documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html#contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">General theory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html">What is a computer cluster?</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#parts-of-a-computing-cluster">Parts of a computing cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#unix">UNIX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#the-workload-manager">The workload manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#processing-data">Processing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#software-on-the-cluster">Software on the cluster</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Systems and services</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Computing infrastructure and policies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#roles-and-authorizations">Roles and authorizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overview-of-available-computing-resources-at-mila">Overview of available computing resources at Mila</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mila-cluster-versus-compute-canada-clusters">Mila cluster versus Compute Canada clusters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#guarantees-about-one-gpu-as-absolute-minimum">Guarantees about one GPU as absolute minimum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#node-profile-description">Node profile description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#special-nodes-and-outliers">Special nodes and outliers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dgx-a100">DGX A100</a></li>
<li class="toctree-l4"><a class="reference internal" href="#power9">Power9</a></li>
<li class="toctree-l4"><a class="reference internal" href="#amd">AMD</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-sharing-policies">Data sharing policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring">Monitoring</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-with-netdata-on-cn-c001">Example with Netdata on cn-c001</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-watching-the-cpu-ram-gpu-usage">Example watching the CPU/RAM/GPU usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-with-mila-dashboard">Example with Mila dashboard</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#storage">Storage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#home">$HOME</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scratch">$SCRATCH</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-tmpdir">$SLURM_TMPDIR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#projects">projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="#archive">$ARCHIVE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#datasets">datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-transmission">Data Transmission</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Extra_compute.html">Computational resources outside of Mila</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How-tos and Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Userguide.html">User’s guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="Handbook.html">AI tooling and methodology handbook</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extras</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Audio_video.html">Audio and video resources at Mila</a></li>
<li class="toctree-l1"><a class="reference internal" href="VSCode.html">Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="IDT.html">Who, what, where is IDT</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MILA Technical Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Computing infrastructure and policies</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mila-iqia/mila-docs/blob/master/docs/Information.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="computing-infrastructure-and-policies">
<span id="infra-info"></span><h1>Computing infrastructure and policies<a class="headerlink" href="#computing-infrastructure-and-policies" title="Permalink to this heading"></a></h1>
<p>This section seeks to provide factual information and policies on the Mila cluster computing environments.</p>
<section id="roles-and-authorizations">
<h2>Roles and authorizations<a class="headerlink" href="#roles-and-authorizations" title="Permalink to this heading"></a></h2>
<p>There are mainly two types of researchers statuses at Mila :</p>
<ol class="arabic simple">
<li><p>Core researchers</p></li>
<li><p>Affiliated researchers</p></li>
</ol>
<p>This is determined by Mila policy. Core researchers have access to the Mila
computing cluster. See your supervisor’s Mila status to know what is your own
status.</p>
</section>
<section id="overview-of-available-computing-resources-at-mila">
<h2>Overview of available computing resources at Mila<a class="headerlink" href="#overview-of-available-computing-resources-at-mila" title="Permalink to this heading"></a></h2>
<p>The Mila cluster is to be used for regular development and relatively small
number of jobs (&lt; 5). It is a heterogeneous cluster. It uses
<span class="xref std std-ref">SLURM</span> to schedule jobs.</p>
<section id="mila-cluster-versus-compute-canada-clusters">
<h3>Mila cluster versus Compute Canada clusters<a class="headerlink" href="#mila-cluster-versus-compute-canada-clusters" title="Permalink to this heading"></a></h3>
<p>There are a lot of commonalities between the Mila cluster and the clusters from
Compute Canada (CC).  At the time being, the CC clusters where we have a large
allocation of resources are <cite>beluga</cite>, <cite>cedar</cite> and <cite>graham</cite>.  We also have
comparable computational resources in the Mila cluster, with more to come.</p>
<p>The main distinguishing factor is that we have more control over our own
cluster than we have over the ones at Compute Canada.  Notably, also, the
compute nodes in the Mila cluster all have unrestricted access to the Internet,
which is not the case in general for CC clusters (although <cite>cedar</cite> does allow
it).</p>
<p>At the current time of this writing (June 2021), Mila students are advised to
use a healthy diet of a mix of Mila and CC clusters.  This is especially true
in times when your favorite cluster is oversubscribed, because you can easily
switch over to a different one if you are used to it.</p>
</section>
<section id="guarantees-about-one-gpu-as-absolute-minimum">
<h3>Guarantees about one GPU as absolute minimum<a class="headerlink" href="#guarantees-about-one-gpu-as-absolute-minimum" title="Permalink to this heading"></a></h3>
<p>There are certain guarantees that the Mila cluster tries to honor when it comes
to giving <em>at minimum</em> one GPU per student, all the time, to be used in
interactive mode. This is strictly better than “one GPU per student on average”
because it’s a floor meaning that, at any time, you should be able to ask for
your GPU, right now, and get it (although it might take a minute for the
request to be processed by SLURM).</p>
<p>Interactive sessions are possible on the CC clusters, and there are generally
special rules that allow you to get resources more easily if you request them
for a very short duration (for testing code before queueing long jobs).  You do
not get the same guarantee as on the Mila cluster, however.</p>
</section>
</section>
<section id="node-profile-description">
<h2>Node profile description<a class="headerlink" href="#node-profile-description" title="Permalink to this heading"></a></h2>
<span class="target" id="node-list"></span><table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 7%" />
<col style="width: 2%" />
<col style="width: 4%" />
<col style="width: 6%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Name</p></th>
<th class="head" colspan="2"><p>GPU</p></th>
<th class="head" rowspan="2"><p>CPUs</p></th>
<th class="head" rowspan="2"><p>Sockets</p></th>
<th class="head" rowspan="2"><p>Cores/Socket</p></th>
<th class="head" rowspan="2"><p>Threads/Core</p></th>
<th class="head" rowspan="2"><p>Memory (GB)</p></th>
<th class="head" rowspan="2"><p>TmpDisk (TB)</p></th>
<th class="head" rowspan="2"><p>Arch</p></th>
<th class="head"><p>Slurm Features</p></th>
</tr>
<tr class="row-even"><th class="head"><p>Model</p></th>
<th class="head"><p>#</p></th>
<th class="head"><p>GPU Arch and Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td colspan="11"><p><span class="h"><h5 style="margin: 5px 0 0 0;">GPU Compute Nodes</h5></span></p></td>
</tr>
<tr class="row-even"><td><p><strong>cn-a[001-011]</strong></p></td>
<td><p>RTX8000</p></td>
<td><p>8</p></td>
<td><p>40</p></td>
<td><p>2</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>384</p></td>
<td><p>3.6</p></td>
<td><p>x86_64</p></td>
<td><p>turing,48gb</p></td>
</tr>
<tr class="row-odd"><td><p><strong>cn-b[001-005]</strong></p></td>
<td><p>V100</p></td>
<td><p>8</p></td>
<td><p>40</p></td>
<td><p>2</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>384</p></td>
<td><p>3.6</p></td>
<td><p>x86_64</p></td>
<td><p>volta,nvlink,32gb</p></td>
</tr>
<tr class="row-even"><td><p><strong>cn-c[001-040]</strong></p></td>
<td><p>RTX8000</p></td>
<td><p>8</p></td>
<td><p>64</p></td>
<td><p>2</p></td>
<td><p>32</p></td>
<td><p>1</p></td>
<td><p>384</p></td>
<td><p>3</p></td>
<td><p>x86_64</p></td>
<td><p>turing,48gb</p></td>
</tr>
<tr class="row-odd"><td colspan="11"><p><span class="h"><h5 style="margin: 5px 0 0 0;">DGX Systems</h5></span></p></td>
</tr>
<tr class="row-even"><td><p><strong>cn-d[001-002]</strong></p></td>
<td><p>A100</p></td>
<td><p>8</p></td>
<td><p>128</p></td>
<td><p>2</p></td>
<td><p>64</p></td>
<td><p>1</p></td>
<td><p>1024</p></td>
<td><p>14</p></td>
<td><p>x86_64</p></td>
<td><p>ampere,nvlink,40gb</p></td>
</tr>
<tr class="row-odd"><td><p><strong>cn-e001</strong></p></td>
<td><p>V100</p></td>
<td><p>8</p></td>
<td><p>40</p></td>
<td><p>2</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>512</p></td>
<td><p>7</p></td>
<td><p>x86_64</p></td>
<td><p>volta,16gb</p></td>
</tr>
<tr class="row-even"><td><p><strong>cn-e[002-003]</strong></p></td>
<td><p>V100</p></td>
<td><p>8</p></td>
<td><p>40</p></td>
<td><p>2</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>512</p></td>
<td><p>7</p></td>
<td><p>x86_64</p></td>
<td><p>volta,32gb</p></td>
</tr>
<tr class="row-odd"><td colspan="11"><p><span class="h"><h5 style="margin: 5px 0 0 0;">Legacy GPU Compute Nodes</h5></span></p></td>
</tr>
<tr class="row-even"><td><p>kepler5</p></td>
<td><p>V100</p></td>
<td><p>2</p></td>
<td><p>16</p></td>
<td><p>2</p></td>
<td><p>4</p></td>
<td><p>2</p></td>
<td><p>256</p></td>
<td><p>3.6</p></td>
<td><p>x86_64</p></td>
<td><p>volta,16gb</p></td>
</tr>
<tr class="row-odd"><td colspan="11"><p><span class="h"><h5 style="margin: 5px 0 0 0;">TITAN RTX</h5></span></p></td>
</tr>
<tr class="row-even"><td><p>rtx[1,3-5,7]</p></td>
<td><p>titanrtx</p></td>
<td><p>2</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>10</p></td>
<td><p>2</p></td>
<td><p>128</p></td>
<td><p>0.93</p></td>
<td><p>x86_64</p></td>
<td><p>turing,24gb</p></td>
</tr>
<tr class="row-odd"><td colspan="11"><p><span class="h"><h5 style="margin: 5px 0 0 0;">POWER9</h5></span></p></td>
</tr>
<tr class="row-even"><td><p>power9[1-2]</p></td>
<td><p>V100</p></td>
<td><p>4</p></td>
<td><p>128</p></td>
<td><p>2</p></td>
<td><p>16</p></td>
<td><p>4</p></td>
<td><p>586</p></td>
<td><p>0.88</p></td>
<td><p>power9</p></td>
<td><p>volta,nvlink,16gb</p></td>
</tr>
</tbody>
</table>
<section id="special-nodes-and-outliers">
<h3>Special nodes and outliers<a class="headerlink" href="#special-nodes-and-outliers" title="Permalink to this heading"></a></h3>
<section id="dgx-a100">
<h4>DGX A100<a class="headerlink" href="#dgx-a100" title="Permalink to this heading"></a></h4>
<p id="dgx-a100-nodes">DGX A100 nodes are NVIDIA appliances with 8 NVIDIA A100 Tensor Core GPUs. Each
GPU has 40 GB of memory, for a total of 320 GB per appliance. The GPUs are
interconnected via 6 NVSwitches which allows 4.8 TB/s bi-directional bandwidth.</p>
<p>In order to run jobs on a DGX A100, add the flags below to your Slurm
commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">gres</span><span class="o">=</span><span class="n">gpu</span><span class="p">:</span><span class="n">a100</span><span class="p">:</span><span class="o">&lt;</span><span class="n">number</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">reservation</span><span class="o">=</span><span class="n">DGXA100</span>
</pre></div>
</div>
</section>
<section id="power9">
<h4>Power9<a class="headerlink" href="#power9" title="Permalink to this heading"></a></h4>
<p id="power9-nodes"><a class="reference external" href="https://en.wikipedia.org/wiki/POWER9">Power9</a> nodes are using a different processor instruction set than Intel and
AMD (x86_64) based nodes. As such you need to setup your environment again
for those nodes specifically.</p>
<ul class="simple">
<li><p>Power9 nodes have 128 threads. (2 processors / 16 cores / 4 way SMT)</p></li>
<li><p>4 x V100 SMX2 (16 GB) with NVLink</p></li>
<li><p>In a Power9 node GPUs and CPUs communicate with each other using NVLink
instead of PCIe. This allow them to communicate quickly between each other.
More on Large Model Support (<a class="reference external" href="https://developer.ibm.com/articles/performance-results-with-lmstf2/">LMS</a>)</p></li>
</ul>
<p>Power9 nodes have the same software stack as the regular nodes and each
software should be included to deploy your environment as on a regular node.</p>
</section>
<section id="amd">
<h4>AMD<a class="headerlink" href="#amd" title="Permalink to this heading"></a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As of August 20 2019 the GPUs had to return back to AMD.  Mila will get
more samples. You can join the <a class="reference external" href="https://mila-umontreal.slack.com/archives/CKV5YKEP6/p1561471261000500">amd</a> slack channels to get the latest
information</p>
</div>
<p>Mila has a few node equipped with <a class="reference external" href="https://www.amd.com/en/products/professional-graphics/instinct-mi50">MI50</a> GPUs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
span.prompt2:before {
  content: " ";
}
</style><span class="prompt1">srun --gres<span class="o">=</span>gpu -c <span class="m">8</span> --reservation<span class="o">=</span>AMD --pty bash</span>
<span class="prompt2"></span>
<span class="prompt2"> first <span class="nb">time</span> setup of AMD stack</span>
<span class="prompt1">conda create -n rocm <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.6</span>
<span class="prompt1">conda activate rocm</span>
<span class="prompt2"></span>
<span class="prompt1">pip install tensorflow-rocm</span>
<span class="prompt1">pip install /wheels/pytorch/torch-1.1.0a0+d8b9d32-cp36-cp36m-linux_x86_64.whl</span>
</pre></div></div></section>
</section>
</section>
<section id="data-sharing-policies">
<h2>Data sharing policies<a class="headerlink" href="#data-sharing-policies" title="Permalink to this heading"></a></h2>
<div class="admonition note" id="acl-note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="Information.html#storage">/network/scratch</a> aims to support
<a class="reference external" href="https://cl-cheat-sheet.readthedocs.io/en/latest/#setfacl">Access Control Lists (ACLs)</a>
to allow collaborative work on rapidly changing data, e.g. work in process
datasets, model checkpoints, etc…</p>
</div>
<p><a class="reference external" href="Information.html#storage">/network/projects</a> aims to offer a collaborative
space for long-term projects. Data that should be kept for a longer period then
90 days can be stored in that location but first a request to <a class="reference external" href="https://it-support.mila.quebec">Mila’s helpdesk</a> has to be made to create the project
directory.</p>
</section>
<section id="monitoring">
<h2>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this heading"></a></h2>
<p>Every compute node on the Mila cluster has a <a class="reference external" href="https://www.netdata.cloud/">Netdata</a>
monitoring daemon allowing you to get a sense of the state of the node.
This information is exposed in two ways:</p>
<ul class="simple">
<li><p>For every node, there is a web interface from Netdata itself at <code class="docutils literal notranslate"><span class="pre">&lt;node&gt;.server.mila.quebec:19999</span></code>.
This is accessible only when using the Mila wifi or through SSH tunnelling.</p>
<ul>
<li><p>SSH tunnelling: on your local machine, run</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-L</span> <span class="pre">19999:&lt;node&gt;.server.mila.quebec:19999</span> <span class="pre">-p</span> <span class="pre">2222</span>
<span class="pre">login.server.mila.quebec</span></code></p></li>
<li><p>or <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-L</span> <span class="pre">19999:&lt;node&gt;.server.mila.quebec:19999</span> <span class="pre">mila</span></code> if you have
already setup your <a class="reference internal" href="Userguide.html#ssh-login"><span class="std std-ref">SSH Login</span></a>,</p></li>
</ul>
</li>
<li><p>then open <a class="reference external" href="http://localhost:19999">http://localhost:19999</a> in your browser.</p></li>
</ul>
</li>
<li><p>The Mila dashboard at <a class="reference external" href="https://dashboard.server.mila.quebec/">dashboard.server.mila.quebec</a>
exposes aggregated statistics with the use of <a class="reference external" href="https://grafana.com/">grafana</a>.
These are collected internally to an instance of <a class="reference external" href="https://prometheus.io/">prometheus</a>.</p></li>
</ul>
<p>In both cases, those graphs are not editable by individual users,
but they provide valuable insight into the state of the whole cluster
or the individual nodes.
One of the important uses is to collect data about the health
of the Mila cluster and to sound the alarm if outages occur
(e.g. if the nodes crash or if GPUs mysteriously become unavailable for SLURM).</p>
<section id="example-with-netdata-on-cn-c001">
<h3>Example with Netdata on cn-c001<a class="headerlink" href="#example-with-netdata-on-cn-c001" title="Permalink to this heading"></a></h3>
<p>For example, if we have a job running on <code class="docutils literal notranslate"><span class="pre">cn-c001</span></code>, we can type
<code class="docutils literal notranslate"><span class="pre">cn-c001.server.mila.quebec:19999</span></code> in a browser address bar and the following
page will appear.</p>
<img alt="monitoring.png" class="align-center" src="_images/monitoring.png" />
</section>
<section id="example-watching-the-cpu-ram-gpu-usage">
<h3>Example watching the CPU/RAM/GPU usage<a class="headerlink" href="#example-watching-the-cpu-ram-gpu-usage" title="Permalink to this heading"></a></h3>
<p>Given that compute nodes are generally shared
with other users who are also running jobs at the same time and
consuming resources, this is not generally a good way to profile your code
in fine details.
However, it can still be a very useful source of information
for getting an idea of whether the machine that you requested is being
used in its full capacity.</p>
<p>Given how expensive the GPUs are, it generally makes sense to try to
make sure that this resources is always kept busy.</p>
<ul class="simple">
<li><dl class="simple">
<dt>CPU</dt><dd><ul>
<li><p>iowait (pink line): High values means your model is waiting on IO a lot (disk or network).</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="monitoring_cpu.png" class="align-center" src="_images/monitoring_cpu.png" />
<ul class="simple">
<li><dl class="simple">
<dt>CPU RAM</dt><dd><ul>
<li><p>You can see how much CPU RAM is being used by your script in practice,
considering the amount that you requested (e.g. <code class="docutils literal notranslate"><span class="pre">`sbatch</span> <span class="pre">--mem=8G</span> <span class="pre">...`</span></code>).</p></li>
<li><p>GPU usage is generally more important to monitor than CPU RAM.
You should not cut it so close to the limit that your experiments randomly fail
because they run out of RAM. However, you should not request blindly 32GB of RAM
when you actually require only 8GB.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="monitoring_ram.png" class="align-center" src="_images/monitoring_ram.png" />
<ul class="simple">
<li><dl class="simple">
<dt>GPU</dt><dd><ul>
<li><p>Monitors the GPU usage using an <a class="reference external" href="https://learn.netdata.cloud/docs/agent/collectors/python.d.plugin/nvidia_smi/">nvidia-smi plugin for Netdata</a>.</p></li>
<li><p>Under the plugin interface, select the GPU number which was allocated to
you. You can figure this out by running <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$SLURM_JOB_GPUS</span></code> on the
allocated node or, if you have the job ID,
<code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">-d</span> <span class="pre">job</span> <span class="pre">YOUR_JOB_ID</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">'GRES'</span></code> and checking <code class="docutils literal notranslate"><span class="pre">IDX</span></code></p></li>
<li><p>You should make sure you use the GPUs to their fullest capacity.</p></li>
<li><p>Select the biggest batch size if possible to increase GPU memory usage and
the GPU computational load.</p></li>
<li><p>Spawn multiple experiments if you can fit many on a single GPU.
Running 10 independent MNIST experiments on a single GPU will probably take
less than 10x the time to run a single one. This assumes that you have more
experiments to run, because nothing is gained by gratuitously running experiments.</p></li>
<li><p>You can request a less powerful GPU and leave the more powerful GPUs
to other researchers who have experiments that can make best use of them.
Sometimes you really just need a k80 and not a v100.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="monitoring_gpu.png" class="align-center" src="_images/monitoring_gpu.png" />
<ul class="simple">
<li><dl class="simple">
<dt>Other users or jobs</dt><dd><ul>
<li><p>If the node seems unresponsive or slow,
it may be useful to check what other tasks are
running at the same time on that node.
This should not be an issue in general,
but in practice it is useful to be able to
inspect this to diagnose certain problems.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="monitoring_users.png" class="align-center" src="_images/monitoring_users.png" />
</section>
<section id="example-with-mila-dashboard">
<h3>Example with Mila dashboard<a class="headerlink" href="#example-with-mila-dashboard" title="Permalink to this heading"></a></h3>
<img alt="mila_dashboard_2021-06-15.png" class="align-center" src="_images/mila_dashboard_2021-06-15.png" />
</section>
</section>
<section id="storage">
<span id="milacluster-storage"></span><h2>Storage<a class="headerlink" href="#storage" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 39%" />
<col style="width: 8%" />
<col style="width: 27%" />
<col style="width: 14%" />
<col style="width: 4%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Path</p></th>
<th class="head"><p>Performance</p></th>
<th class="head"><p>Usage</p></th>
<th class="head"><p>Quota (Space/Files)</p></th>
<th class="head"><p>Backup</p></th>
<th class="head"><p>Auto-cleanup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/network/datasets/</span></code></p></td>
<td><p>High</p></td>
<td><ul class="simple">
<li><p>Curated raw datasets (read only)</p></li>
</ul>
</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$HOME</span></code> or <code class="docutils literal notranslate"><span class="pre">/home/mila/&lt;u&gt;/&lt;username&gt;/</span></code></p></td>
<td><p>Low</p></td>
<td><ul class="simple">
<li><p>Personal user space</p></li>
<li><p>Specific libraries, code, binaries</p></li>
</ul>
</td>
<td><p>100GB/1000K</p></td>
<td><p>Daily</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> or <code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;/</span></code></p></td>
<td><p>High</p></td>
<td><ul class="simple">
<li><p>Temporary job results</p></li>
<li><p>Processed datasets</p></li>
<li><p>Optimized for small Files</p></li>
</ul>
</td>
<td><p>no</p></td>
<td><p>no</p></td>
<td><p>90 days</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code></p></td>
<td><p>Highest</p></td>
<td><ul class="simple">
<li><p>High speed disk for temporary job
results</p></li>
</ul>
</td>
<td><p>4TB/-</p></td>
<td><p>no</p></td>
<td><p>at job end</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/network/projects/&lt;groupname&gt;/</span></code></p></td>
<td><p>Fair</p></td>
<td><ul class="simple">
<li><p>Shared space to facilitate
collaboration between researchers</p></li>
<li><p>Long-term project storage</p></li>
</ul>
</td>
<td><p>200GB/1000K</p></td>
<td><p>Daily</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$ARCHIVE</span></code> or <code class="docutils literal notranslate"><span class="pre">/network/archive/&lt;u&gt;/&lt;username&gt;/</span></code></p></td>
<td><p>Low</p></td>
<td><ul class="simple">
<li><p>Long-term personal storage</p></li>
</ul>
</td>
<td><p>500GB</p></td>
<td><p>no</p></td>
<td><p>no</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> file system is backed up once a day. For any file
restoration request, file a request to <a class="reference external" href="https://it-support.mila.quebec">Mila’s IT support</a> with the path to the file or directory to
restore, with the required date.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently there is no backup system for any other file systems of
the Mila cluster. Storage local to personal computers, Google Drive and other
related solutions should be used to backup important data</p>
</div>
<section id="home">
<h3>$HOME<a class="headerlink" href="#home" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$HOME</span></code> is appropriate for codes and libraries which are small and read once,
as well as the experimental results that would be needed at a later time (e.g.
the weights of a network referenced in a paper).</p>
<p>Quotas are enabled on <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> for both disk capacity (blocks) and number of
files (inodes). The limits for blocks and inodes are respectively 100GiB and 1
million per user. The command to check the quota usage from a login node is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">beegfs-ctl --cfgFile<span class="o">=</span>/etc/beegfs/home.d/beegfs-client.conf --getquota --uid <span class="nv">$USER</span></span>
</pre></div></div></section>
<section id="scratch">
<h3>$SCRATCH<a class="headerlink" href="#scratch" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> can be used to store processed datasets, work in progress datasets
or temporary job results. Its block size is optimized for small files which
minimizes the performance hit of working on extracted datasets.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Auto-cleanup</strong>: this file system is cleared on a weekly basis,
files not used for more than 90 days will be deleted.</p>
</div>
</section>
<section id="slurm-tmpdir">
<h3>$SLURM_TMPDIR<a class="headerlink" href="#slurm-tmpdir" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> points to the local disk of the node on which a job is
running. It should be used to copy the data on the node at the beginning of the
job and write intermediate checkpoints. This folder is cleared after each job.</p>
</section>
<section id="projects">
<h3>projects<a class="headerlink" href="#projects" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">projects</span></code> can be used for collaborative projects. It aims to ease the
sharing of data between users working on a long-term project.</p>
<p>Quotas are enabled on <code class="docutils literal notranslate"><span class="pre">projects</span></code> for both disk capacity (blocks) and number
of files (inodes). The limits for blocks and inodes are respectively 200GiB and
1 million per user and per group.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is possible to request higher quota limits if the project requires
it. File a request to <a class="reference external" href="https://it-support.mila.quebec">Mila’s IT support</a>.</p>
</div>
</section>
<section id="archive">
<h3>$ARCHIVE<a class="headerlink" href="#archive" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$ARCHIVE</span></code> purpose is to store data other than datasets that has to be kept
long-term (e.g.  generated samples, logs, data relevant for paper submission).</p>
<p><code class="docutils literal notranslate"><span class="pre">$ARCHIVE</span></code> is only available on the <strong>login</strong> nodes. Because this file system
is tuned for large files, it is recommended to archive your directories. For
example, to archive the results of an experiment in
<code class="docutils literal notranslate"><span class="pre">$SCRATCH/my_experiment_results/</span></code>, run the commands below from a login node:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">cd</span> <span class="nv">$SCRATCH</span></span>
<span class="prompt1">tar cJf <span class="nv">$ARCHIVE</span>/my_experiment_results.tar.xz --xattrs my_experiment_results</span>
</pre></div></div><p>Disk capacity quotas are enabled on <code class="docutils literal notranslate"><span class="pre">$ARCHIVE</span></code>. The soft limit per user is
500GB, the hard limit is 550GB. The grace time is 7 days. This means that one
can use more than 500GB for 7 days before the file system enforces quota.
However, it is not possible to use more than 550GB.
The command to check the quota usage from a login node is <cite>df</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">df -h <span class="nv">$ARCHIVE</span></span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is <strong>NO</strong> backup of this file system.</p>
</div>
</section>
<section id="datasets">
<h3>datasets<a class="headerlink" href="#datasets" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">datasets</span></code> contains curated datasets to the benefit of the Mila community.
To request the addition of a dataset or a preprocessed dataset you think could
benefit the research of others, you can fill <a class="reference external" href="https://forms.gle/vDVwD2rZBmYHENgZA">this form</a>.</p>
<p>Datasets in <code class="docutils literal notranslate"><span class="pre">datasets/restricted</span></code> are restricted and require an explicit
request to gain access. Please <a class="reference external" href="https://milaquebec.freshdesk.com/a/tickets/new">submit a support ticket</a> mentioning the dataset’s
access group (ex.: <code class="docutils literal notranslate"><span class="pre">scannet_users</span></code>), your cluster’s username and the
approbation of the group owner. You can find the dataset’s access group by
listing the content of <code class="docutils literal notranslate"><span class="pre">/network/datasets/restricted</span></code> with the <a class="reference external" href="https://cli-cheatsheet.readthedocs.io/en/latest/#ls">ls command</a>.</p>
</section>
</section>
<section id="data-transmission">
<h2>Data Transmission<a class="headerlink" href="#data-transmission" title="Permalink to this heading"></a></h2>
<p>Multiple methods can be used to transfer data to/from the cluster:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span> <span class="pre">--bwlimit=10mb</span></code>; this is the favored method since the bandwidth can
be limited to prevent impacting the usage of the cluster: <a class="reference external" href="https://cl-cheat-sheet.readthedocs.io/en/latest/#rsync">rsync</a></p></li>
<li><p>Compute Canada: <a class="reference external" href="https://docs.computecanada.ca/wiki/Globus">Globus</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Theory_cluster.html" class="btn btn-neutral float-left" title="What is a computer cluster?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Extra_compute.html" class="btn btn-neutral float-right" title="Computational resources outside of Mila" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<script type="text/javascript">
  window.onload = function() {
      $(".toggle > *").hide();
      $(".toggle .header").show();
      $(".toggle .header").click(function() {
          $(this).parent().children().not(".header").toggle(400);
          $(this).parent().children(".header").toggleClass("open");
      })
  };
</script>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>